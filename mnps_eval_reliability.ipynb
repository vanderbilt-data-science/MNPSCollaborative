{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/MNPSCollaborative/blob/main/mnps_eval_reliability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNPS Evaluation and Reliability Testing Framework\n",
        "> A notebook to help with the experimental design framework for the project.  \n",
        "> DSI DSSG + MNPS  \n",
        "> August 12, 2025  \n",
        "> Drafted by Wayne Birch - [contact him](wayne.birch@mnps.org) for questions, code update needs, or other questions about the notebook!\n",
        "\n",
        "This notebook builds off the starting point for the mini Hackathon with Metro Nashville Public Schools (MNPS) and the VU Data Science Institute (VU DSI). You aren't constrained to what is in this notebook, and please feel free to use your creativity to deliver the best solution\n",
        "\n",
        "## **1** | Competition Parameters\n",
        "* **Outcome and evaluation**: Participants will be evaluated on the performance of their provided solution on the holdout set. Importantly, judges must be able to easily run the submitted code on the new dataset.\n",
        "* **Objective**: The overall objective is to create a system which best automatically, reproducibly, and reliably categorizes jobs according to the parameters set forth by MNPS. A few suggestions are provided on parameters that you can vary if you're thinking about achievable changes in 2.5 hours\n"
      ],
      "metadata": {
        "id": "Vxr9nUDPhuC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2** | Environment Setup\n",
        "Again, you're completely free to just download this notebook, create a local virtual environment and get to coding in your favorite IDE. We provide this code just as a rapid method to get started, and focus our efforts on implementation through Google Colab.\n",
        "\n",
        "### **2a** | API Key Setup\n",
        "#### **2a.1** | Access\n",
        "The DSI has provided you an API key which can access **some** of the OpenAI models. These include:\n",
        "* All versions of gpt-4o\n",
        "* All versions of gpt-4.1\n",
        "* All versions of o3-mini\n",
        "\n",
        "Vector store upload, web search, code interpreter, and other functionality outside of the Chat Completions and Messages API is **not** supported. If you really want to use these things, you will have to make a good and cost-supported argument. If you don't feel like arguing, you can also utilize your own OpenAI API key.\n",
        "\n",
        "#### **2a.2** | API Keys in Google Colab\n",
        "To use your API key, click on the key icon (looks sort of like üîë) in the left sidebar.  Under **Name**, add `OPENAI_API_KEY`. Under **Value**, paste your API key. Your API key is a jumble of numbers and letters, maybe even other symbols. Click the slider checkbox to enable **Notebook access** (so your notebook will grab these values without asking you).  \n",
        "\n",
        "### **2b** | Runtime setup\n",
        "We're going to install some packages in your environment so that you have access to the code functionality. If you need more packages, install more packages. Install **only** packages you trust."
      ],
      "metadata": {
        "id": "zYYyUWf7ltd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "BMAhEZJdq3y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "\n",
        "# set OpenAI API key environment variable using Google Colab\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "q9RrxxQNPb-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3** | The Data\n",
        "\n",
        "The current prompt is a two-step prompt that is successful through the ChatGPT interface. It requires two types of data:\n",
        "* The data to be classified\n",
        "* Supporting resources\n",
        "\n",
        "We need to read all of this in. Let's grab it and use it. The first thing you'll do is just straight up download a zip file of all of this information.\n",
        "\n",
        "You can download all of the reference files from the link provided, then upload in the sidebar. You'll then unzip the directory using the code below.\n",
        "\n",
        "Click on the folder icon in the left sidebar (kinda looks like this üóÇÔ∏è) and you'll see all the files there. We'll read them in.\n"
      ],
      "metadata": {
        "id": "vdVEU9lGlnC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/2025u-mnps-minihackathon.zip"
      ],
      "metadata": {
        "id": "iaKoFAightj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resources_dir_prefix = '/content/2025u-mnps-minihackathon/prompt-resources/'\n",
        "roles_lookup = pd.read_csv(resources_dir_prefix+\"MNPS Roles.csv\")\n",
        "determinants = pd.read_csv(resources_dir_prefix+\"Competency Extended Descriptions.csv\", encoding='latin1')\n",
        "ksac_table = pd.read_csv(resources_dir_prefix+\"MNPS KSACs.csv\")\n",
        "korn_ferry = pd.read_csv(resources_dir_prefix+\"Korn_Ferry Lominger 38 Competencies.csv\", encoding='latin1')"
      ],
      "metadata": {
        "id": "xn87fRq-ViIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4** | The Prompts\n",
        "\n",
        "What we have here is a direct prompt to get the response that we're looking for. We'll make this happen directly using the OpenAI Chat Completions API. Note that you can use other APIs as you like."
      ],
      "metadata": {
        "id": "m0bzttINsCQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4stx_ljahbqt"
      },
      "outputs": [],
      "source": [
        "zero_shot_prompt = \\\n",
        "\"\"\" Objective: Evaluate and group jobs from the \"Job Description Export Specialists.xlsx\" file based on similarities in job functions, not job titles.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Compare all jobs against each other using the attributes listed in the file: Education, Work Experience, Licenses/Certifications, Essential Functions, Knowledge, Skills, Abilities, and Position Summary.\n",
        "- Compare each job with reference sources using the same attributes. I have attached the reference sources for you.\n",
        "- Group jobs based on similarities into:\n",
        "  - Major role groupings (e.g., Specialist, Analyst, Manager)\n",
        "  - Minor sub-groupings (e.g., Specialist I, II, III, IV) - not to exceed level IV\n",
        "- Use the MNPS Roles and MNPS KSACs documents to help you determine major role groupings.\n",
        "- Use the remaining documents to help you clarify subtle differences in role groupings and sub-groupings.\n",
        "- Use a more qualitative, holistic assessment focused on functional alignment with KSACs rather than a quantitative scoring approach with defined complexity metrics\n",
        "\n",
        "Output Format:\n",
        "\n",
        "- Create a table with the following columns:\n",
        "  - Original Job Title\n",
        "  - New Job Title\n",
        "  - Major Role Group\n",
        "  - Minor Sub-Group\n",
        "  - Justification for Grouping\n",
        "\n",
        "- Provide an accompanying narrative explaining the rationale behind the groupings and any notable patterns or insights discovered during the analysis.\n",
        "\n",
        "Job Title Convention:\n",
        "\n",
        "- Follow the format: \"[Function] [Role] [Level]\" (e.g., \"Collections Specialist II\", \"Accounts Payable Specialist III\")\n",
        "\n",
        "Additional Guidelines:\n",
        "\n",
        "- Ensure all sources used are cited properly.\n",
        "- Focus on the nature of the work performed rather than just the job titles.\n",
        "- Consider the complexity of tasks, level of responsibility, and required competencies when determining groupings.\n",
        "- Provide clear explanations for why each job was classified as it was, referencing specific job attributes and external benchmarks.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of asking for a table output, we will use **structured outputs**. Though this is a common approach for the outputs of LLMs/AI systems, you can learn more about this on [OpenAI's structured output documentation](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses). Note that you can find this information on almost all LLM/AI platform or package providers."
      ],
      "metadata": {
        "id": "v2fP0nG0NqB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class JobClassification(BaseModel):\n",
        "    \"\"\"Represents the classification of a job based on its functions.\"\"\"\n",
        "    job_title_original: str = Field(..., description=\"The original job title as provided in the input data using the job title convention specified.\")\n",
        "    new_job_title: str = Field(..., description=\"The proposed new job title based on the classification using the job title convention specified.\")\n",
        "    major_role_group: str = Field(..., description=\"The major grouping of the job based on its functional role (e.g., Specialist, Analyst, Manager).\")\n",
        "    minor_sub_group: str = Field(..., description=\"The minor sub-grouping within the major role group (e.g., Specialist I, II, III, IV).\")\n",
        "    grouping_justification: str = Field(..., description=\"The justification for placing the job in the specific major and minor groups, referencing job attributes and relevant documents.\")"
      ],
      "metadata": {
        "id": "cs3EhFP4OLFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JobClassificationTable(BaseModel):\n",
        "  \"\"\"The table classification and overall commentary on the groupings provided by the AI system.\"\"\"\n",
        "  job_classification_table: List[JobClassification] = Field(..., description=\"The table of job classifications.\")\n",
        "  narrative_rationale: str = Field(..., description=\"The narrative commentary on the groupings provided by the AI system.\")"
      ],
      "metadata": {
        "id": "23L4liisNluw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create classifications using OpenAI. Of note here is:\n",
        "* The **developer** prompt - this is the \"system prompt\" or \"custom instructions\" for the model. This determines the overall behavior of the model.\n",
        "* The **user** prompt - this is what we send to the model like when we're chatting with ChatGPT."
      ],
      "metadata": {
        "id": "oRPxg9JEPDt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create openAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Create messages to send\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": zero_shot_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"Classify the following job description: [Paste Job Description Here]\"} # Replace with actual job description\n",
        "]\n",
        "\n",
        "# Assuming JobClassification and zero_shot_prompt are defined in the preceding code\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o\", # Or another available model\n",
        "    messages=messages,\n",
        "    temperature=1,\n",
        "    max_tokens=1000,\n",
        "    response_format=JobClassificationTable\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "E9ff_JExPDMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at response\n",
        "response.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "cAzjnoIeQdBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make this into a table using pandas!"
      ],
      "metadata": {
        "id": "DnuQIiz9c_Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_dict = dict(*response.choices[0].message.parsed.job_classification_table)\n",
        "response_dict"
      ],
      "metadata": {
        "id": "LLa01IgYdSkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see outputs\n",
        "pd.DataFrame(response_dict, index=[0])"
      ],
      "metadata": {
        "id": "1NoOUqyocxu2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}