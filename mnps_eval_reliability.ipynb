{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/MNPSCollaborative/blob/Parsing-and-Schema-Bug-Fix-Section-4-and-Add-Batch-Run%2C-Adjudication%2C-and-Scoring/mnps_eval_reliability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNPS Evaluation and Reliability Testing Framework\n",
        "> A notebook to help with the experimental design framework for the project.  \n",
        "> DSI DSSG + MNPS  \n",
        "> August 12, 2025  \n",
        "> Drafted by Wayne Birch - [contact him](wayne.birch@mnps.org) for questions, code update needs, or other questions about the notebook!\n",
        "\n",
        "This notebook builds off the starting point for the mini Hackathon with Metro Nashville Public Schools (MNPS) and the VU Data Science Institute (VU DSI). You aren't constrained to what is in this notebook, and please feel free to use your creativity to deliver the best solution\n",
        "\n",
        "## **1** | Overview\n",
        "* **Project Summary**: We‚Äôre checking whether the AI Assistant assigns the **right MNPS job title** (and major/minor role) when it reads a job description. We‚Äôll use **a random mix of MNPS internal and external descriptions** from [New Sample_08.07.2025.csv](https://github.com/vanderbilt-data-science/MNPSCollaborative/blob/main/New%20Sample_08.07.2025.csv). For each record, the Assistant outputs a title and roles; then **a human evaluator** reviews the prediction and marks it right or wrong.\n",
        "\n",
        "  Our go/no-go rule is simple: the model **passes only** if‚Äîeven after accounting for normal sampling wiggle room‚Äîits **true accuracy is at least 90%**. We measure that with a conservative 95% statistical check. We‚Äôll also look at performance separately for **internal vs. external** descriptions and across **major role groups** (e.g., Specialist, Analyst, Director). We only exclude items with an empty/too-short Position Summary; other missing fields are okay because the model (and human reviewer) can infer what‚Äôs needed.\n",
        "\n",
        "  If we find recurring miss-patterns (like confusing seniority or over-weighting job titles vs. duties), we‚Äôll **tune the prompt** and rerun. The notebook produces a clean adjudication sheet for the human reviewer, calculates accuracy with confidence intervals, and prints a clear **PASS/REJECT**decision.\n",
        "\n",
        "* **Method Details**: Design. Prospective evaluation of an AI Assistant that classifies job descriptions into an MNPS top-1 job title (primary endpoint) and major/minor role (secondary endpoints).\n",
        "\n",
        "  **Dataset.** We evaluate on a **random sample** of both internal MNPS and external job descriptions from [New Sample_08.07.2025.csv](https://github.com/vanderbilt-data-science/MNPSCollaborative/blob/main/New%20Sample_08.07.2025.csv). The **only** a priori exclusion is records lacking sufficient text in the **Position Summary** field, which is required for adjudication. Missing or sparse content in other fields (e.g., Education, Work Experience, Licenses/Certifications, KSAs) is not exclusionary because those signals can often be inferred from the Position Summary or recovered by the AI model during classification.\n",
        "\n",
        "  **Model and Outputs.** For each description, the Assistant returns a structured JSON with: predicted title, major role, minor role, a 0‚Äì1 confidence score, a brief rationale, and a prompt version tag. JSON is schema-checked before scoring.\n",
        "\n",
        "    **Ground truth.** A human evaluator reviews each prediction. Where used for formal reporting, we recommend dual independent review with adjudication and reporting **inter-rater reliability** (e.g., Cohen‚Äôs Œ∫‚â•0.75), but the protocol supports single-evaluator adjudication for prompt-tuning cycles.\n",
        "\n",
        "  **Primary outcome and acceptance criterion.** Top-1 title accuracy with a two-sided Clopper‚ÄìPearson 95% confidence interval. We accept the model if the lower bound ‚â• 0.90. This rule is pre-specified and applied once per evaluation run.\n",
        "\n",
        "  **Secondary outcomes.** (i) Title accuracy by Source (Internal vs. External) and by Major role group, each with Wilson 95% intervals for readability; (ii) Major/minor correctness rates; (iii) Error taxonomy counts (e.g., ‚Äúseniority misread,‚Äù ‚Äúwrong job family,‚Äù ‚Äúduties overweighted/underweighted‚Äù).\n",
        "\n",
        "  **Analysis plan.** The notebook calculates overall accuracy and confidence intervals, prints a PASS/REJECT decision, and exports subgroup tables and error buckets for prompt iteration. An optional checkpoint table reports the minimum number correct required for the acceptance lower-bound at common sample sizes. Prompt changes are versioned; re-tests are run on the full set after targeted fixes informed by the error taxonomy.\n",
        "\n",
        "  **Bias & limitations.** External job descriptions vary in style and detail; misclassification risk rises when licensure or scope signals are missing. To mitigate, the prompt explicitly weights Essential Functions, Education/Experience, and Licenses/Certifications over job title wording and brand terms. Results generalize to descriptions similar in content and detail to the sample.\n",
        "\n",
        "  **Reproducibility.** The notebook fixes the analysis rule (exact 95% CI lower-bound ‚â•0.90), logs (record_id, prompt_version, model_json, timestamp), exports the human adjudication sheet and scored results, and supports re-runs with updated prompts."
      ],
      "metadata": {
        "id": "Vxr9nUDPhuC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1** | One-Glance Notebook Pipeline\n",
        "0 ‚Üí 1 ‚Üí 2.1 ‚Üí 3 ‚Üí 4 ‚Üí 5 ‚Üí 6 ‚Üí (Human) ‚Üí 7 ‚Üí 8 ‚Üí 9 ‚Üí 10 ‚Üí 11\n",
        "\n",
        "0. Overview: Why and how (plain-English + methods).\n",
        "\n",
        "1. Config & acceptance rule: Set knobs and the PASS rule (exact 95% LB ‚â• 0.90).\n",
        "\n",
        "2. Configuration and Acceptance Rule\n",
        "\n",
        "  2.1 Eval Sample Load: Load New Sample_08.07.2025.csv, exclude only thin Position Summaries, assign Record_ID.\n",
        "\n",
        "3. The Data: Supporting resources & how to run the model (keep as-is).\n",
        "\n",
        "4. The Prompts: Dev prompt + Pydantic schema + parsing; (keep as-is, with tiny fixes).\n",
        "\n",
        "5. Batch run: Run model for each record in df_eval ‚Üí model_outputs.csv.\n",
        "\n",
        "6. Adjudication sheet: Merge source text + predictions ‚Üí MNPS_Adjudication_Sheet.csv.\n",
        "\n",
        "  **(Human Step):** Reviewer fills Y/N + final labels ‚Üí MNPS_Adjudication_Filled.csv.\n",
        "\n",
        "7. Score & decision: Read filled sheet; compute accuracy + exact CI; PASS/REJECT; Wilson by subgroups; error buckets.\n",
        "\n",
        "8. Subgroups & diagnostics: Breakouts (Internal/External, Major), top error categories (prompt fixes).\n",
        "\n",
        "9. Helper (min successes): Quick table of ‚Äúmin correct to reach LB‚â•0.90‚Äù for common n.\n",
        "\n",
        "10. Exports: Save subgroup tables and error buckets for reporting.\n",
        "\n",
        "11. Next actions: If REJECT, patch prompt (targeted), bump version, re-run (first failures, then full)."
      ],
      "metadata": {
        "id": "eZrntiEO0ZB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2** | Notebook Section Breakdown and Continuity\n",
        "**0. Overview (Markdown)**\n",
        "  \n",
        "  * Purpose: Narrative that frames the evaluation; what ‚ÄúPASS‚Äù means.\n",
        "  * Inputs: None.\n",
        "  * Outputs: Documentation only.\n",
        "  * Who: You (authoring).\n",
        "\n",
        "**1. Config and Acceptance rule (Code)**\n",
        "\n",
        "  * Purpose: Central place for all constants (e.g., ACCEPT_LB=0.90, ALPHA=0.05, file paths).\n",
        "  * Inputs: None (you set values).\n",
        "  * Outputs: Globals used downstream (ACCEPT_LB, MIN_SUMMARY_CHARS, filenames).\n",
        "  * Who: You (tuning).\n",
        "\n",
        "**2. Evaluation Sample Load (Code)**\n",
        "    \n",
        "  * Purpose: Create a clean evaluation table.\n",
        "  *Inputs: New Sample_08.07.2025.csv.\n",
        "  *Logic: Only exclude rows with empty/too-short Position Summary; keep others even if sparse.\n",
        "  *Outputs: df_eval with a stable Record_ID.\n",
        "  *Who: Code (automated).\n",
        "\n",
        "**3. The Data (Keep as-is)**\n",
        "\n",
        "  * Purpose: Any resources/instructions needed to run the actual model (KSACs, roles, etc.).\n",
        "  * Inputs: Reference files you use in prompts/logic.\n",
        "  * Outputs: Context/material for Section 4.\n",
        "  * Who: You & readers (instructions), model (as needed).\n",
        "\n",
        "**4. The Prompts (Keep as-is; add tiny fixes if needed)**\n",
        "\n",
        "  * Purpose: Developer prompt, Pydantic schema, OpenAI call, structured parse.\n",
        "  * Inputs: One record‚Äôs text (from df_eval) per call; your prompt.\n",
        "  * Outputs: Structured prediction(s) (title/major/minor/confidence/rationale), parsable by code.\n",
        "  * Who: Model + your code.\n",
        "\n",
        "  * Tiny fixes (where to place):\n",
        "  ‚Ä¢ Add from typing import List, Optional right under your Pydantic import.\n",
        "  ‚Ä¢ Replace any dict(*parsed.job_classification_table) with [item.model_dump() for item in parsed.job_classification_table].\n",
        "\n",
        "**5. Batch model run (Code)**\n",
        "\n",
        "  * Purpose: Loop over all df_eval rows, call Section-4 model code, collect outputs.\n",
        "  * Inputs: df_eval, prompt/schema from Section 4.\n",
        "  * Outputs: results_df (in-memory) and model_outputs.csv on disk.\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**6. Build adjudication sheet (Code)**\n",
        "\n",
        "  * Purpose: Create the file the human will grade.\n",
        "  * Inputs: df_eval (source text) + results_df (predictions).\n",
        "  * Outputs: MNPS_Adjudication_Sheet.csv with blank Y/N and final label fields.\n",
        "  * Who: Code produces; Human fills.\n",
        "\n",
        "**(Human) Adjudication**\n",
        "\n",
        "  * Purpose: Gold standard decision per row.\n",
        "  * Inputs: MNPS_Adjudication_Sheet.csv.\n",
        "  * Outputs: MNPS_Adjudication_Filled.csv (same columns, Y/N + final labels completed).\n",
        "  * Who: Human reviewer(s).\n",
        "\n",
        "**7. Ingest Adjudication and Score (Code)**\n",
        "\n",
        "  * Purpose: Apply the pre-specified acceptance rule and show diagnostics\n",
        "  * Inputs: MNPS_Adjudication_Filled.csv.\n",
        "  * Calculations:\n",
        "      **Primary:** top-1 title accuracy with exact two-sided 95% CI; PASS if LB ‚â• 0.90.\n",
        "      **Secondary:** Wilson CIs by Source and (if present) Major; error categories.\n",
        "  * Outputs: Console decision + summary; in-memory tables (by_src, by_major, err_counts).\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**8. Subgroups and Diagnostics (Code)**\n",
        "\n",
        "  * Purpose: Breakouts and error taxonomy for targeted prompt fixes.\n",
        "  * Inputs: Adjudicated table from 7.\n",
        "  * Outputs: Summaries for reporting/iteration (and later exported in 10).\n",
        "  * Who: Code (automated), You (interpretation).\n",
        "\n",
        "**9. Helper: min k for LB‚â•target (Code)**\n",
        "\n",
        "  * Purpose: Quick reference of ‚Äúhow many correct do we need at size n?‚Äù\n",
        "  * Inputs: Target LB, alpha.\n",
        "  * Outputs: Printed thresholds for common n (100, 200, 300, ‚Ä¶).\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**10. Export Tables (Code)**\n",
        "\n",
        "  * Purpose: Persist the subgroup and error summaries.\n",
        "  * Inputs: Tables from 7‚Äì8.\n",
        "  * Outputs: results_by_source.csv, results_by_major.csv (if present), results_error_buckets.csv.\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**11. Next Actions (Markdown)**\n",
        "\n",
        "  * Purpose: The iteration plan if REJECT (or what to lock if PASS).\n",
        "  * Inputs: Diagnostics from 7‚Äì8.\n",
        "  * Outputs: To-dos: patch prompt, bump PROMPT_VERSION, re-run fails ‚Üí full set, re-score.\n",
        "  * Who: You (decision + edits).\n",
        "\n"
      ],
      "metadata": {
        "id": "K1dyMtnvJEbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3** | Run Order Checklist\n",
        "1. Run Sections 1 ‚Üí 2.1 to load & gate the sample.\n",
        "2. (Optional) glance at 3 for resources.\n",
        "3. Confirm 4 runs cleanly on one row (schema + parse).\n",
        "4. Run 5 to batch predictions ‚Üí model_outputs.csv.\n",
        "5. Run 6 to create MNPS_Adjudication_Sheet.csv; send to reviewer.\n",
        "6. After review, place MNPS_Adjudication_Filled.csv next to the notebook.\n",
        "7. Run 7 (decision) + 8‚Äì10 (diagnostics & exports).\n",
        "8. If REJECT, follow 11 (patch prompt, bump version, re-run).\n",
        "\n"
      ],
      "metadata": {
        "id": "tfdWVEKkP_iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4** | Artifact Map (what files get created)\n",
        "* model_outputs.csv ‚Üí Section 5 (raw model predictions).\n",
        "* MNPS_Adjudication_Sheet.csv ‚Üí Section 6 (for the human).\n",
        "* MNPS_Adjudication_Filled.csv ‚Üí Human returns this (input to Section 7).\n",
        "* results_by_source.csv, results_by_major.csv, results_error_buckets.csv ‚Üí Section 10 (reporting)."
      ],
      "metadata": {
        "id": "3sBw0As6Qfli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2** | Configuration and Acceptance Rule\n",
        "Again, you're completely free to just download this notebook, create a local virtual environment and get to coding in your favorite IDE. We provide this code just as a rapid method to get started, and focus our efforts on implementation through Google Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "zYYyUWf7ltd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "BMAhEZJdq3y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "\n",
        "# set OpenAI API key environment variable using Google Colab\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "q9RrxxQNPb-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIG ---\n",
        "ACCEPT_LB = 0.90   # lower bound target\n",
        "ALPHA = 0.05       # 95% confidence\n",
        "PRIMARY_FIELD = \"title\"  # primary metric: top-1 title accuracy\n",
        "\n",
        "# Data location: use ONE of the following\n",
        "DATA_URL = \"https://raw.githubusercontent.com/vanderbilt-data-science/MNPSCollaborative/main/New%20Sample_08.07.2025.csv\"\n",
        "DATA_PATH = None   # e.g., \"./New Sample_08.07.2025.csv\" if local\n",
        "\n",
        "# Required input columns (from the CSV headers provided)\n",
        "REQUIRED_COLS = [\n",
        "    \"Job Description Name\",\n",
        "    \"Position Summary\",\n",
        "    \"Education\",\n",
        "    \"Work Experience\",\n",
        "    \"Essential Functions\",\n",
        "    \"Licenses and Certifications\",\n",
        "    \"Knowledge, Skills and Abilities\",\n",
        "    \"Source\"   # Internal / External\n",
        "]\n",
        "\n",
        "# Allowed labels for model outputs (extend if needed)\n",
        "ALLOWED_MAJORS = [\n",
        "    \"Specialist\", \"Analyst\", \"Director\", \"Manager\", \"Technician\", \"Coordinator\"\n",
        "]\n",
        "\n",
        "# Output filenames\n",
        "MODEL_OUT_CSV = \"model_outputs.csv\"\n",
        "ADJ_SHEET_CSV = \"MNPS_Adjudication_Sheet.csv\"  # to be filled by human\n",
        "ADJ_FILLED_CSV = \"MNPS_Adjudication_Filled.csv\"  # human‚Äëcompleted file name (you will upload/point to it later)"
      ],
      "metadata": {
        "id": "oxFiwmBMbaMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1** | Evaluation Sample Load and Only-Exclusion Rule\n",
        "This section is the s the ‚Äúload and gatekeeper‚Äù for the evaluation set.\n",
        "\n",
        "* It loads the sample CSV (New Sample_08.07.2025.csv) from the repo (or a local path if you switch DATA_PATH).\n",
        "* Enforces the only exclusion rule: drops rows whose Position Summary is empty/too short (‚â§ MIN_SUMMARY_CHARS).\n",
        "* Normalizes (trims) the Position Summary text.\n",
        "* Creates a stable ID (Record_ID) if the file doesn‚Äôt already have one.\n",
        "* Hands off a clean table called df_eval for the later ‚Äúrun model ‚Üí adjudicate ‚Üí score‚Äù steps."
      ],
      "metadata": {
        "id": "MIyRvtRisuCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EVALUATION SAMPLE ---\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/vanderbilt-data-science/MNPSCollaborative/main/New%20Sample_08.07.2025.csv\"\n",
        "MIN_SUMMARY_CHARS = 10  # only a priori exclusion\n",
        "\n",
        "df_eval = pd.read_csv(DATA_URL)\n",
        "if \"Position Summary\" not in df_eval.columns:\n",
        "    raise ValueError(\"Missing required 'Position Summary' column.\")\n",
        "\n",
        "# Only exclusion: thin/empty Position Summary\n",
        "df_eval[\"Position Summary\"] = df_eval[\"Position Summary\"].astype(str).str.strip()\n",
        "df_eval = df_eval[df_eval[\"Position Summary\"].str.len() > MIN_SUMMARY_CHARS].copy()\n",
        "\n",
        "# Stable ID for joining later\n",
        "if \"Record_ID\" not in df_eval.columns:\n",
        "    df_eval.insert(0, \"Record_ID\", np.arange(1, len(df_eval)+1))\n",
        "\n",
        "print(f\"Evaluation rows loaded: n={len(df_eval)}\")\n",
        "df_eval.head(3)"
      ],
      "metadata": {
        "id": "ogkMGAvJtCzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3** | The Data\n",
        "\n",
        "The current prompt is a two-step prompt that is successful through the ChatGPT interface. It requires two types of data:\n",
        "* The data to be classified\n",
        "* Supporting resources\n",
        "\n",
        "We need to read all of this in. Let's grab it and use it. The first thing you'll do is just straight up download a zip file of all of this information.\n",
        "\n",
        "You can download all of the reference files from the link provided, then upload in the sidebar. You'll then unzip the directory using the code below.\n",
        "\n",
        "Click on the folder icon in the left sidebar (kinda looks like this üóÇÔ∏è) and you'll see all the files there. We'll read them in.\n"
      ],
      "metadata": {
        "id": "vdVEU9lGlnC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/2025u-mnps-minihackathon.zip"
      ],
      "metadata": {
        "id": "iaKoFAightj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resources_dir_prefix = '/content/2025u-mnps-minihackathon/prompt-resources/'\n",
        "roles_lookup = pd.read_csv(resources_dir_prefix+\"MNPS Roles.csv\")\n",
        "determinants = pd.read_csv(resources_dir_prefix+\"Competency Extended Descriptions.csv\", encoding='latin1')\n",
        "ksac_table = pd.read_csv(resources_dir_prefix+\"MNPS KSACs.csv\")\n",
        "korn_ferry = pd.read_csv(resources_dir_prefix+\"Korn_Ferry Lominger 38 Competencies.csv\", encoding='latin1')"
      ],
      "metadata": {
        "id": "xn87fRq-ViIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4** | The Prompts\n",
        "\n",
        "What we have here is a direct prompt to get the response that we're looking for. We'll make this happen directly using the OpenAI Chat Completions API. Note that you can use other APIs as you like."
      ],
      "metadata": {
        "id": "m0bzttINsCQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4stx_ljahbqt"
      },
      "outputs": [],
      "source": [
        "zero_shot_prompt = \\\n",
        "\"\"\" Objective: Evaluate and group jobs from the \"Job Description Export Specialists.xlsx\" file based on similarities in job functions, not job titles.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Compare all jobs against each other using the attributes listed in the file: Education, Work Experience, Licenses/Certifications, Essential Functions, Knowledge, Skills, Abilities, and Position Summary.\n",
        "- Compare each job with reference sources using the same attributes. I have attached the reference sources for you.\n",
        "- Group jobs based on similarities into:\n",
        "  - Major role groupings (e.g., Specialist, Analyst, Manager)\n",
        "  - Minor sub-groupings (e.g., Specialist I, II, III, IV) - not to exceed level IV\n",
        "- Use the MNPS Roles and MNPS KSACs documents to help you determine major role groupings.\n",
        "- Use the remaining documents to help you clarify subtle differences in role groupings and sub-groupings.\n",
        "- Use a more qualitative, holistic assessment focused on functional alignment with KSACs rather than a quantitative scoring approach with defined complexity metrics\n",
        "\n",
        "Output Format:\n",
        "\n",
        "- Create a table with the following columns:\n",
        "  - Original Job Title\n",
        "  - New Job Title\n",
        "  - Major Role Group\n",
        "  - Minor Sub-Group\n",
        "  - Justification for Grouping\n",
        "\n",
        "- Provide an accompanying narrative explaining the rationale behind the groupings and any notable patterns or insights discovered during the analysis.\n",
        "\n",
        "Job Title Convention:\n",
        "\n",
        "- Follow the format: \"[Function] [Role] [Level]\" (e.g., \"Collections Specialist II\", \"Accounts Payable Specialist III\")\n",
        "\n",
        "Additional Guidelines:\n",
        "\n",
        "- Ensure all sources used are cited properly.\n",
        "- Focus on the nature of the work performed rather than just the job titles.\n",
        "- Consider the complexity of tasks, level of responsibility, and required competencies when determining groupings.\n",
        "- Provide clear explanations for why each job was classified as it was, referencing specific job attributes and external benchmarks.\n",
        "\n",
        "CONSTRAINTS:\n",
        "1) Prioritize signals in this order: Essential Functions > Education/Experience > Licensure/Certifications. Ignore employer branding and title fluff unless supported by duties/scope.\n",
        "2) Restrict major_role_group to one of: Specialist, Analyst, Director, Manager, Technician, Coordinator.\n",
        "3) Always return confidence_0to1 as a conservative float in [0, 1].\n",
        "4) Echo back the record_id provided in the input.\n",
        "\n",
        "CONSTRAINTS (Title-Mention Hygiene):\n",
        "5) Ignore any explicit job title strings that appear in Position Summary or Essential Functions (e.g., ‚ÄúDirector of Operations‚Äù, ‚ÄúSenior Manager‚Äù). Treat them as uninformative branding.\n",
        "6) When such strings occur, mentally rewrite them to ‚Äúthis role‚Äù and base decisions solely on duties/scope, Education/Experience, and Licensure, per MNPS rules.\n",
        "7) Do NOT quote or paraphrase those title strings in grouping_justification; justify using duties, scope/leadership signals, required credentials, and KSAC alignment.\n",
        "8) If duties/scope contradict an in-text title string, follow duties/scope and required credentials ‚Äî never the string.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of asking for a table output, we will use **structured outputs**. Though this is a common approach for the outputs of LLMs/AI systems, you can learn more about this on [OpenAI's structured output documentation](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses). Note that you can find this information on almost all LLM/AI platform or package providers."
      ],
      "metadata": {
        "id": "v2fP0nG0NqB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional  # ‚Üê add this line\n",
        "\n",
        "class JobClassification(BaseModel):\n",
        "    \"\"\"Represents the classification of a single job.\"\"\"\n",
        "    record_id: int = Field(..., description=\"Record_ID from the evaluation CSV.\")\n",
        "    job_title_original: str = Field(..., description=\"Original job title from input.\")\n",
        "    new_job_title: str = Field(..., description=\"Proposed MNPS-style title.\")\n",
        "    major_role_group: str = Field(..., description=\"One of: Specialist, Analyst, Director, Manager, Technician, Coordinator.\")\n",
        "    minor_sub_group: str = Field(..., description=\"Level (e.g., I, II, III, IV) or blank if not applicable.\")\n",
        "    grouping_justification: str = Field(..., description=\"One-sentence rationale anchored on Essential Functions, Education/Experience, and Licensure.\")\n",
        "    confidence_0to1: float = Field(..., ge=0.0, le=1.0, description=\"Self-rated confidence, conservative calibration.\")\n",
        "\n",
        "class JobClassificationTable(BaseModel):\n",
        "    job_classification_table: List[JobClassification] = Field(..., description=\"One entry per input job.\")\n",
        "    narrative_rationale: str = Field(..., description=\"Optional narrative across the set.\")\n"
      ],
      "metadata": {
        "id": "cs3EhFP4OLFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create classifications using OpenAI. Of note here is:\n",
        "* The **developer** prompt - this is the \"system prompt\" or \"custom instructions\" for the model. This determines the overall behavior of the model.\n",
        "* The **user** prompt - this is what we send to the model like when we're chatting with ChatGPT."
      ],
      "metadata": {
        "id": "oRPxg9JEPDt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create openAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Create messages to send\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": zero_shot_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"Classify the following job description: [Paste Job Description Here]\"} # Replace with actual job description\n",
        "]\n",
        "\n",
        "# Assuming JobClassification and zero_shot_prompt are defined in the preceding code\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o\", # Or another available model\n",
        "    messages=messages,\n",
        "    temperature=0.2, #changed from 1\n",
        "    max_tokens=1000,\n",
        "    response_format=JobClassificationTable,   # ‚Üê this\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "E9ff_JExPDMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at response\n",
        "response.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "cAzjnoIeQdBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make this into a table using pandas!"
      ],
      "metadata": {
        "id": "DnuQIiz9c_Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the structured result (optional)\n",
        "parsed = response.choices[0].message.parsed\n",
        "\n",
        "# Extract the table (list of JobClassification objects)\n",
        "table = parsed.job_classification_table\n",
        "\n",
        "# If you expect exactly one classification, single-row dict:\n",
        "first_item = table[0].model_dump()     # .model_dump() for Pydantic v2\n",
        "# first_item -> {'job_title_original': ..., 'new_job_title': ..., ...}\n",
        "\n",
        "# Or, if you want a DataFrame of all rows:\n",
        "rows = [item.model_dump() for item in table]\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df.head()"
      ],
      "metadata": {
        "id": "1NoOUqyocxu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5** | Batch Model Run on the Evaluation Sample\n",
        "\n",
        "Runs the AI once per row in df_eval (your filtered New Sample_08.07.2025.csv), using your Section-4 prompt/schema.\n",
        "* **Inputs:** df_eval, your zero_shot_prompt, model name, and PROMPT_VERSION.\n",
        "* **Outputs:** A tidy predictions table with one row per record:\n",
        "\n",
        "    * Record_ID, Model_Prompt_Version, Model_Run_Timestamp\n",
        "\n",
        "    * Model_Pred_Title, Model_Pred_Major, Model_Pred_Minor\n",
        "\n",
        "    * Model_Confidence_0to1, Model_Rationale_Short\n",
        "It saves this as model_outputs.csv. (No scoring yet‚Äîjust model outputs.)"
      ],
      "metadata": {
        "id": "IkgwG4FrvxF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 | Batch model run (Code)\n",
        "import os, re, datetime as dt\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "PROMPT_VERSION = \"v1\"          # bump when you change the prompt\n",
        "MODEL_NAME = \"gpt-4o\"\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# --- Title mention masking (prevents leakage from in-line titles) ---\n",
        "TITLE_TERMS = r\"(senior|sr\\.?|junior|jr\\.?|lead|chief|principal|head|assistant|associate|executive\\s+director|vice\\s+president|vp|director|manager|specialist|analyst|technician|coordinator|officer|administrator|architect|engineer|supervisor)\"\n",
        "\n",
        "def mask_title_mentions(text: str) -> str:\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return text or \"\"\n",
        "    # e.g., \"Senior Director of Operations\", \"VP, Sales\", \"Manager for Logistics\"\n",
        "    pattern = rf\"(?i)\\b(?:the\\s+)?(?:{TITLE_TERMS})\\b(?:\\s*(?:of|for|,)?\\s*[A-Z][\\w/&\\-, ]+)?\"\n",
        "    return re.sub(pattern, \"this role\", text)\n",
        "\n",
        "def classify_one(row):\n",
        "    # Mask only the fields where ad-hoc titles usually appear\n",
        "    ps = mask_title_mentions(row.get('Position Summary', ''))\n",
        "    ef = mask_title_mentions(row.get('Essential Functions', ''))\n",
        "\n",
        "    user_payload = f\"\"\"\n",
        "Record_ID: {int(row['Record_ID'])}\n",
        "Original Job Title: {row.get('Job Description Name','')}\n",
        "Position Summary: {ps}\n",
        "Education: {row.get('Education','')}\n",
        "Work Experience: {row.get('Work Experience','')}\n",
        "Licenses and Certifications: {row.get('Licenses and Certifications','')}\n",
        "Knowledge, Skills and Abilities: {row.get('Knowledge, Skills and Abilities','')}\n",
        "Essential Functions: {ef}\n",
        "\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"developer\", \"content\": zero_shot_prompt},\n",
        "        {\"role\": \"user\", \"content\": \"Classify the following job description. Output 1 item in job_classification_table.\\n\" + user_payload}\n",
        "    ]\n",
        "\n",
        "    resp = client.beta.chat.completions.parse(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "        max_tokens=1000,\n",
        "        response_format=JobClassificationTable,\n",
        "    )\n",
        "    parsed = resp.choices[0].message.parsed.job_classification_table[0].model_dump()\n",
        "\n",
        "    # Normalize to evaluation columns\n",
        "    return {\n",
        "        \"Record_ID\": parsed[\"record_id\"],\n",
        "        \"Model_Prompt_Version\": PROMPT_VERSION,\n",
        "        \"Model_Run_Timestamp\": dt.datetime.utcnow().isoformat(),\n",
        "        \"Model_Pred_Title\": parsed[\"new_job_title\"],\n",
        "        \"Model_Pred_Major\": parsed[\"major_role_group\"],\n",
        "        \"Model_Pred_Minor\": parsed[\"minor_sub_group\"],\n",
        "        \"Model_Confidence_0to1\": float(parsed[\"confidence_0to1\"]),\n",
        "        \"Model_Rationale_Short\": parsed[\"grouping_justification\"],\n",
        "    }\n",
        "\n",
        "# df_eval should already exist from your earlier load step\n",
        "pred_rows = [classify_one(r) for _, r in df_eval.iterrows()]\n",
        "results_df = pd.DataFrame(pred_rows)\n",
        "results_df.to_csv(\"model_outputs.csv\", index=False)\n",
        "print(\"Saved model_outputs.csv\")\n",
        "results_df.head(3)\n"
      ],
      "metadata": {
        "id": "zfQ3cgVOv9vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1** | Bridge Outputs ‚Üí Standard Columns\n",
        "This cell normalizes what the batch run produced into the columns downstream cells expect. It supports two sources:\n",
        "* a parsed `response` still in memory\n",
        "* an existing DataFrame named `results_df` already built in Section 4/5"
      ],
      "metadata": {
        "id": "ffre-O_eUqPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 ‚Äî Bridge outputs ‚Üí standard columns\n",
        "import pandas as pd, numpy as np, datetime as dt\n",
        "\n",
        "# This cell normalizes whatever Section 4 (or your batch run) produced into\n",
        "# the columns downstream cells expect. It supports two sources:\n",
        "#   (A) a parsed `response` still in memory\n",
        "#   (B) an existing DataFrame named `results_df` you already built in Section 4/5\n",
        "\n",
        "def _to_rows_from_parsed(parsed_table):\n",
        "    # parsed_table: list of Pydantic objects\n",
        "    rows = []\n",
        "    for item in parsed_table:\n",
        "        if hasattr(item, \"model_dump\"):\n",
        "            rows.append(item.model_dump())  # Pydantic v2\n",
        "        else:\n",
        "            # Fallback if you somehow have dict-like items already\n",
        "            rows.append(dict(item))\n",
        "    return rows\n",
        "\n",
        "# Try (A): pull from a `response` variable left by Section 4\n",
        "_rows = None\n",
        "try:\n",
        "    _parsed = response.choices[0].message.parsed\n",
        "    _rows = _to_rows_from_parsed(_parsed.job_classification_table)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Otherwise (B): use an existing DataFrame `results_df` (if you already built it)\n",
        "if _rows is not None:\n",
        "    _raw_df = pd.DataFrame(_rows)\n",
        "elif \"results_df\" in globals() and isinstance(results_df, pd.DataFrame):\n",
        "    _raw_df = results_df.copy()\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        \"B1 bridge: couldn't find Section-4/5 outputs.\\n\"\n",
        "        \"Ensure you either keep `response` in memory (after parse) or have a DataFrame named `results_df`.\"\n",
        "    )\n",
        "\n",
        "# Map Section-4/5 columns ‚Üí standardized evaluation columns\n",
        "_cands = {\n",
        "    \"Record_ID\":             [\"record_id\"],  # if absent, we'll backfill on merge with df_eval\n",
        "    \"Model_Pred_Title\":      [\"new_job_title\", \"pred_title\"],\n",
        "    \"Model_Pred_Major\":      [\"major_role_group\", \"pred_major\"],\n",
        "    \"Model_Pred_Minor\":      [\"minor_sub_group\", \"pred_minor\"],\n",
        "    \"Model_Rationale_Short\": [\"grouping_justification\", \"rationale\"],\n",
        "    \"Model_Confidence_0to1\": [\"confidence_0to1\", \"confidence\", \"score\"],\n",
        "    \"Job_Title_Original\":    [\"job_title_original\", \"original_title\"],\n",
        "}\n",
        "\n",
        "def _pick(df, options, default=np.nan):\n",
        "    for col in options:\n",
        "        if col in df.columns:\n",
        "            return df[col]\n",
        "    return default\n",
        "\n",
        "# If you track prompt version elsewhere, set it here\n",
        "PROMPT_VERSION = globals().get(\"PROMPT_VERSION\", \"v1\")\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"Record_ID\":             _pick(_raw_df, _cands[\"Record_ID\"]),\n",
        "    \"Model_Prompt_Version\":  PROMPT_VERSION,\n",
        "    \"Model_Run_Timestamp\":   dt.datetime.utcnow().isoformat(),\n",
        "    \"Model_Pred_Title\":      _pick(_raw_df, _cands[\"Model_Pred_Title\"]),\n",
        "    \"Model_Pred_Major\":      _pick(_raw_df, _cands[\"Model_Pred_Major\"]),\n",
        "    \"Model_Pred_Minor\":      _pick(_raw_df, _cands[\"Model_Pred_Minor\"]),\n",
        "    \"Model_Confidence_0to1\": _pick(_raw_df, _cands[\"Model_Confidence_0to1\"]),\n",
        "    \"Model_Rationale_Short\": _pick(_raw_df, _cands[\"Model_Rationale_Short\"]),\n",
        "    \"Job_Title_Original\":    _pick(_raw_df, _cands[\"Job_Title_Original\"]),\n",
        "})\n",
        "\n",
        "# Persist for downstream cells (optional but convenient)\n",
        "results_df.to_csv(\"model_outputs.csv\", index=False)\n",
        "print(\"Normalized Section-4/5 outputs ‚Üí results_df (and wrote model_outputs.csv). Preview:\")\n",
        "results_df.head(3)\n"
      ],
      "metadata": {
        "id": "5jatPQdGVSud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6** | Build Adjudication Sheet for the Human\n",
        "\n",
        "Joins the **source text** (Position Summary, etc.) with the **model outputs** so a human can mark correctness.\n",
        "* **Inputs:** df_eval + results_df from Step 5.\n",
        "* **Outputs:** MNPS_Adjudication_Sheet.csv with:\n",
        "\n",
        "  * Source columns (Record_ID, Source, Job Description Name, Position Summary, Education, Work Experience, Essential Functions, Licenses, KSAs)\n",
        "\n",
        "  * Model columns from Step 5\n",
        "\n",
        "  * Empty columns for the reviewer to fill:\n",
        "Adj_Correct_Title (Y/N), Adj_Correct_Major (Y/N), Adj_Correct_Minor (Y/N), Adj_Final_Title, Adj_Final_Major, Adj_Final_Minor, Adj_Error_Category, Adj_Notes\n",
        "This is the single artifact you hand to the adjudicator."
      ],
      "metadata": {
        "id": "QgO6T2R0wB3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 | Generate adjudication sheet (Code)\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# ---- Inputs ----\n",
        "MODEL_OUTPUTS_CSV = \"model_outputs.csv\"  # produced in Section 5\n",
        "ADJ_SHEET_CSV     = \"MNPS_Adjudication_Sheet.csv\"\n",
        "\n",
        "# df_eval should still be in memory. If not, set EVAL_SOURCE_CSV to reload it:\n",
        "# EVAL_SOURCE_CSV = \"New Sample_08.07.2025.csv\"\n",
        "# df_eval = pd.read_csv(EVAL_SOURCE_CSV)\n",
        "\n",
        "# Defensive: ensure expected ID column exists and is integer\n",
        "if \"Record_ID\" not in df_eval.columns:\n",
        "    raise ValueError(\"df_eval must contain 'Record_ID'.\")\n",
        "\n",
        "df_eval[\"Record_ID\"] = df_eval[\"Record_ID\"].astype(int)\n",
        "\n",
        "# Load model outputs\n",
        "if not os.path.exists(MODEL_OUTPUTS_CSV):\n",
        "    raise FileNotFoundError(\"model_outputs.csv not found. Run Section 5 first.\")\n",
        "model_df = pd.read_csv(MODEL_OUTPUTS_CSV)\n",
        "model_df[\"Record_ID\"] = model_df[\"Record_ID\"].astype(int)\n",
        "\n",
        "# Select lightweight context for reviewers (trim long text)\n",
        "def trim(x, n=800):\n",
        "    s = (x or \"\")\n",
        "    return (s[:n] + \" ‚Ä¶\") if len(s) > n else s\n",
        "\n",
        "context_cols = [\n",
        "    \"Record_ID\",\n",
        "    \"Job Description Name\",\n",
        "    \"Position Summary\",\n",
        "    \"Education\",\n",
        "    \"Work Experience\",\n",
        "    \"Licenses and Certifications\",\n",
        "    \"Knowledge, Skills and Abilities\",\n",
        "    \"Essential Functions\",\n",
        "]\n",
        "for c in context_cols:\n",
        "    if c not in df_eval.columns:\n",
        "        df_eval[c] = \"\"  # create blanks if missing\n",
        "\n",
        "adj = df_eval[context_cols].copy()\n",
        "adj[\"Position Summary\"] = adj[\"Position Summary\"].map(lambda s: trim(str(s), 1200))\n",
        "adj[\"Essential Functions\"] = adj[\"Essential Functions\"].map(lambda s: trim(str(s), 1200))\n",
        "\n",
        "# Merge predictions\n",
        "keep_pred_cols = [\n",
        "    \"Model_Prompt_Version\",\n",
        "    \"Model_Run_Timestamp\",\n",
        "    \"Model_Pred_Title\",\n",
        "    \"Model_Pred_Major\",\n",
        "    \"Model_Pred_Minor\",\n",
        "    \"Model_Confidence_0to1\",\n",
        "    \"Model_Rationale_Short\",\n",
        "]\n",
        "for c in keep_pred_cols:\n",
        "    if c not in model_df.columns:\n",
        "        model_df[c] = \"\"\n",
        "\n",
        "out = adj.merge(model_df[[\"Record_ID\"] + keep_pred_cols], on=\"Record_ID\", how=\"left\")\n",
        "\n",
        "# Optional: keep source if present in df_eval (Internal/External)\n",
        "if \"Source\" in df_eval.columns:\n",
        "    out = out.merge(df_eval[[\"Record_ID\", \"Source\"]], on=\"Record_ID\", how=\"left\")\n",
        "\n",
        "# ---- Blank adjudication fields for the human reviewer ----\n",
        "# Recommended categories: Wrong Family, Wrong Seniority/Level, KSAC Misweight, Licensure Misread,\n",
        "# Industry Context Missing, Ambiguous Input, Title Leakage, Other\n",
        "review_cols = {\n",
        "    \"Adjudicator_Initials\": \"\",\n",
        "    \"Reviewed_On_YYYYMMDD\": \"\",\n",
        "    \"Human_Title_Correct_YN\": \"\",   # Y / N\n",
        "    \"Human_Major_Correct_YN\": \"\",   # Y / N\n",
        "    \"Human_Minor_Correct_YN\": \"\",   # Y / N\n",
        "    \"Gold_Title\": \"\",               # fill if Model_Pred_Title is wrong\n",
        "    \"Gold_Major\": \"\",               # fill if Model_Pred_Major is wrong\n",
        "    \"Gold_Minor\": \"\",               # fill if Model_Pred_Minor is wrong\n",
        "    \"Error_Category\": \"\",           # pick from list above\n",
        "    \"Adjudication_Notes\": \"\",\n",
        "}\n",
        "for k, v in review_cols.items():\n",
        "    out[k] = v\n",
        "\n",
        "# Write CSV\n",
        "out.to_csv(ADJ_SHEET_CSV, index=False)\n",
        "print(f\"Wrote adjudication sheet: {ADJ_SHEET_CSV} ({len(out)} rows)\")\n",
        "\n",
        "# Show a peek\n",
        "out.head(3)\n"
      ],
      "metadata": {
        "id": "VM5LM8xFwRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7** | Ingest Adjudication and Compute PASS/REJECT\n",
        "\n",
        "Reads the reviewer‚Äôs completed sheet and applies the acceptance rule.\n",
        "* **Inputs:** MNPS_Adjudication_Filled.csv (same columns as the sheet from Step 6, now filled in).\n",
        "  * **Process:**\n",
        "\n",
        "    * Normalizes Y/N ‚Üí booleans; if Y/N missing, falls back to comparing Model_* vs Adj_Final_*.\n",
        "\n",
        "    * Computes top-1 title accuracy and its two-sided Clopper‚ÄìPearson 95% CI.\n",
        "\n",
        "    * Decision: **PASS** if the exact 95% lower bound ‚â• 0.90, else **REJECT.**\n",
        "\n",
        "    * **Diagnostics:**\n",
        "\n",
        "      * **By Source** (Internal/External) with Wilson 95% CIs.\n",
        "\n",
        "      * **Top Error Categories** (from Adj_Error_Category) to guide prompt fixes.\n",
        "* **Outputs:** Console printout with n, correct_titles, accuracy, exact/Wilson CIs, and the PASS/REJECT decision; plus subgroup and error-bucket summaries. (You can export these to CSV if you like; the earlier ‚Äúexport‚Äù cell handles that.)\n"
      ],
      "metadata": {
        "id": "lVsDl6zDwXGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 | Ingest adjudication & score (Code)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "import os, sys, subprocess\n",
        "\n",
        "ADJ_SHEET_CSV = \"MNPS_Adjudication_Sheet.csv\"\n",
        "SCORED_DETAIL_CSV = \"MNPS_Scored_Detail.csv\"\n",
        "SCORECARD_CSV = \"MNPS_Scorecard.csv\"\n",
        "\n",
        "ALPHA = 0.05\n",
        "ACCEPT_LB = 0.90  # Acceptance rule: pass if exact 95% LB ‚â• 0.90\n",
        "\n",
        "# Try to import SciPy for exact Clopper‚ÄìPearson; install if needed (Colab-friendly)\n",
        "try:\n",
        "    from scipy.stats import beta\n",
        "except Exception:\n",
        "    print(\"Installing SciPy for exact CI...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"scipy\"])\n",
        "    from scipy.stats import beta\n",
        "\n",
        "def clopper_pearson_ci(k: int, n: int, alpha: float = 0.05):\n",
        "    \"\"\"Exact two-sided Clopper‚ÄìPearson CI for a binomial proportion.\"\"\"\n",
        "    if n == 0:\n",
        "        return (0.0, 1.0)\n",
        "    if k == 0:\n",
        "        lb = 0.0\n",
        "    else:\n",
        "        lb = beta.ppf(alpha/2, k, n - k + 1)\n",
        "    if k == n:\n",
        "        ub = 1.0\n",
        "    else:\n",
        "        ub = beta.ppf(1 - alpha/2, k + 1, n - k)\n",
        "    return (float(lb), float(ub))\n",
        "\n",
        "def wilson_ci(k: int, n: int, alpha: float = 0.05):\n",
        "    \"\"\"Wilson (score) interval for a binomial proportion.\"\"\"\n",
        "    if n == 0:\n",
        "        return (0.0, 1.0)\n",
        "    from math import erf, erfc\n",
        "    # For 95%, z ‚âà 1.96; we can compute z from alpha via SciPy, but we‚Äôll hardcode for simplicity\n",
        "    # If you change ALPHA, swap to: from scipy.stats import norm; z = norm.ppf(1 - alpha/2)\n",
        "    z = 1.96 if abs(alpha - 0.05) < 1e-9 else 1.96\n",
        "    p = k / n\n",
        "    denom = 1 + z**2 / n\n",
        "    center = (p + z**2/(2*n)) / denom\n",
        "    half = (z * sqrt((p*(1-p) + z**2/(4*n)) / n)) / denom\n",
        "    return (max(0.0, center - half), min(1.0, center + half))\n",
        "\n",
        "def normalize_title(s: str) -> str:\n",
        "    s = (s or \"\").strip().lower()\n",
        "    # light normalization\n",
        "    s = ''.join(ch for ch in s if ch.isalnum() or ch.isspace())\n",
        "    s = ' '.join(s.split())\n",
        "    return s\n",
        "\n",
        "def yn_to_bool(v):\n",
        "    if isinstance(v, str):\n",
        "        t = v.strip().lower()\n",
        "        if t in (\"y\", \"yes\", \"true\", \"1\"):\n",
        "            return True\n",
        "        if t in (\"n\", \"no\", \"false\", \"0\"):\n",
        "            return False\n",
        "    if isinstance(v, (int, float)) and not pd.isna(v):\n",
        "        return bool(v)\n",
        "    return np.nan  # missing\n",
        "\n",
        "def min_correct_for_lb(n: int, alpha: float = 0.05, target_lb: float = 0.90):\n",
        "    \"\"\"Small helper: minimum number correct k such that exact LB ‚â• target_lb.\"\"\"\n",
        "    best = None\n",
        "    for k in range(n + 1):\n",
        "        lb, _ = clopper_pearson_ci(k, n, alpha)\n",
        "        if lb >= target_lb:\n",
        "            best = k\n",
        "            break\n",
        "    return best\n",
        "\n",
        "# ---- Load adjudication sheet ----\n",
        "if not os.path.exists(ADJ_SHEET_CSV):\n",
        "    raise FileNotFoundError(\"MNPS_Adjudication_Sheet.csv not found. Create it in Section 6.\")\n",
        "\n",
        "adj = pd.read_csv(ADJ_SHEET_CSV)\n",
        "\n",
        "# Create boolean flags from reviewer entries.\n",
        "# Primary rule: if Human_Title_Correct_YN is given ‚Üí use it.\n",
        "# Fallback: if Gold_Title filled, compare to Model_Pred_Title (case-insensitive, normalized).\n",
        "adj[\"Human_Title_Correct\"] = adj.get(\"Human_Title_Correct_YN\", \"\").map(yn_to_bool)\n",
        "\n",
        "need_auto = adj[\"Human_Title_Correct\"].isna()\n",
        "if \"Gold_Title\" in adj.columns:\n",
        "    auto_cmp = (\n",
        "        normalize_title(adj[\"Gold_Title\"])\n",
        "        == normalize_title(adj.get(\"Model_Pred_Title\", \"\"))\n",
        "    )\n",
        "    adj.loc[need_auto, \"Human_Title_Correct\"] = auto_cmp[need_auto]\n",
        "\n",
        "# Similar for major/minor (optional secondary metrics)\n",
        "for col_yn, col_gold, col_pred, out_col in [\n",
        "    (\"Human_Major_Correct_YN\", \"Gold_Major\", \"Model_Pred_Major\", \"Human_Major_Correct\"),\n",
        "    (\"Human_Minor_Correct_YN\", \"Gold_Minor\", \"Model_Pred_Minor\", \"Human_Minor_Correct\"),\n",
        "]:\n",
        "    if col_yn in adj.columns:\n",
        "        adj[out_col] = adj[col_yn].map(yn_to_bool)\n",
        "    else:\n",
        "        adj[out_col] = np.nan\n",
        "    need_auto = adj[out_col].isna()\n",
        "    if col_gold in adj.columns:\n",
        "        auto_cmp = (\n",
        "            normalize_title(adj[col_gold])\n",
        "            == normalize_title(adj.get(col_pred, \"\"))\n",
        "        )\n",
        "        adj.loc[need_auto, out_col] = auto_cmp[need_auto]\n",
        "\n",
        "# Keep only rows that can be scored for title\n",
        "scorable = adj[\"Human_Title_Correct\"].isin([True, False])\n",
        "scored = adj[scorable].copy()\n",
        "\n",
        "n = len(scored)\n",
        "k = int(scored[\"Human_Title_Correct\"].sum()) if n > 0 else 0\n",
        "acc = (k / n) if n else float(\"nan\")\n",
        "cp_lb, cp_ub = clopper_pearson_ci(k, n, alpha=ALPHA)\n",
        "wil_lb, wil_ub = wilson_ci(k, n, alpha=ALPHA)\n",
        "\n",
        "decision = \"PASS\" if cp_lb >= ACCEPT_LB else \"REJECT\"\n",
        "\n",
        "print(\"=== MNPS Evaluation ‚Äî Primary Endpoint (Top-1 Title Accuracy) ===\")\n",
        "print(f\"Scored records: {n}\")\n",
        "print(f\"Correct titles: {k}\")\n",
        "print(f\"Accuracy: {acc:.4f}  ({acc*100:.1f}%)\")\n",
        "print(f\"Exact 95% CI (Clopper‚ÄìPearson): [{cp_lb:.4f}, {cp_ub:.4f}]\")\n",
        "print(f\"Wilson 95% CI (for dashboards): [{wil_lb:.4f}, {wil_ub:.4f}]\")\n",
        "print(f\"Acceptance rule: PASS if exact LB ‚â• {ACCEPT_LB:.2f}\")\n",
        "print(f\"Decision: *** {decision} ***\")\n",
        "\n",
        "# Helpful: print threshold (min correct to pass) for this n\n",
        "k_min = min_correct_for_lb(n, alpha=ALPHA, target_lb=ACCEPT_LB)\n",
        "if k_min is not None:\n",
        "    print(f\"Minimum correct to pass at n={n}: {k_min} (you have {k})\")\n",
        "\n",
        "# ---- Optional subgroup diagnostics ----\n",
        "def subgroup_report(frame: pd.DataFrame, label: str):\n",
        "    if frame.empty:\n",
        "        return\n",
        "    n = len(frame)\n",
        "    k = int(frame[\"Human_Title_Correct\"].sum())\n",
        "    acc = k / n if n else float(\"nan\")\n",
        "    lb, ub = clopper_pearson_ci(k, n, alpha=ALPHA)\n",
        "    print(f\"[{label}] n={n}, acc={acc:.3f}, exact95=[{lb:.3f}, {ub:.3f}]\")\n",
        "\n",
        "print(\"\\n--- Subgroups (if present) ---\")\n",
        "if \"Source\" in scored.columns:\n",
        "    for s, g in scored.groupby(\"Source\"):\n",
        "        subgroup_report(g, f\"Source={s}\")\n",
        "if \"Model_Pred_Major\" in scored.columns:\n",
        "    for s, g in scored.groupby(\"Model_Pred_Major\"):\n",
        "        subgroup_report(g, f\"Pred_Major={s}\")\n",
        "if \"Gold_Major\" in scored.columns:\n",
        "    for s, g in scored.groupby(\"Gold_Major\"):\n",
        "        subgroup_report(g, f\"Gold_Major={s}\")\n",
        "\n",
        "# ---- Save scored detail & scorecard ----\n",
        "scored[\"Exact95_LB\"] = cp_lb\n",
        "scored[\"Exact95_UB\"] = cp_ub\n",
        "scored[\"Wilson95_LB\"] = wil_lb\n",
        "scored[\"Wilson95_UB\"] = wil_ub\n",
        "scored[\"Decision\"] = decision\n",
        "\n",
        "scored.to_csv(SCORED_DETAIL_CSV, index=False)\n",
        "\n",
        "scorecard = pd.DataFrame([{\n",
        "    \"Timestamp_UTC\": pd.Timestamp.utcnow().isoformat(),\n",
        "    \"Prompt_Version\": scored.get(\"Model_Prompt_Version\", pd.Series(dtype=str)).mode().iloc[0] if \"Model_Prompt_Version\" in scored.columns and not scored[\"Model_Prompt_Version\"].empty else \"\",\n",
        "    \"Model_Name\": \"gpt-4o\",\n",
        "    \"N_Scored\": n,\n",
        "    \"K_Correct\": k,\n",
        "    \"Accuracy\": acc,\n",
        "    \"Exact95_LB\": cp_lb,\n",
        "    \"Exact95_UB\": cp_ub,\n",
        "    \"Wilson95_LB\": wil_lb,\n",
        "    \"Wilson95_UB\": wil_ub,\n",
        "    \"Accept_LB\": ACCEPT_LB,\n",
        "    \"Alpha\": ALPHA,\n",
        "    \"Decision\": decision,\n",
        "    \"Min_K_to_Pass_for_this_N\": k_min\n",
        "}])\n",
        "scorecard.to_csv(SCORECARD_CSV, index=False)\n",
        "\n",
        "print(f\"\\nSaved: {SCORED_DETAIL_CSV} and {SCORECARD_CSV}\")\n"
      ],
      "metadata": {
        "id": "mBN3WZf6wfKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}