{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/MNPSCollaborative/blob/Parsing-and-Schema-Bug-Fix-Section-4-and-Add-Batch-Run%2C-Adjudication%2C-and-Scoring/mnps_eval_reliability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNPS Evaluation and Reliability Testing Framework\n",
        "> A notebook to help with the experimental design framework for the project.  \n",
        "> DSI DSSG + MNPS  \n",
        "> August 12, 2025  \n",
        "> Drafted by Wayne Birch - [contact him](wayne.birch@mnps.org) for questions, code update needs, or other questions about the notebook!\n",
        "\n",
        "This notebook builds off the starting point for the mini Hackathon with Metro Nashville Public Schools (MNPS) and the VU Data Science Institute (VU DSI). You aren't constrained to what is in this notebook, and please feel free to use your creativity to deliver the best solution\n",
        "\n",
        "## **1** | Overview\n",
        "* **Project Summary**: Weâ€™re checking whether the AI Assistant assigns the **right MNPS job title** (and major/minor role) when it reads a job description. Weâ€™ll use **a random mix of MNPS internal and external descriptions** from [New Sample_08.07.2025.csv](https://github.com/vanderbilt-data-science/MNPSCollaborative/blob/main/New%20Sample_08.07.2025.csv). For each record, the Assistant outputs a title and roles; then **a human evaluator** reviews the prediction and marks it right or wrong.\n",
        "\n",
        "  Our go/no-go rule is simple: the model **passes only** ifâ€”even after accounting for normal sampling wiggle roomâ€”its **true accuracy is at least 90%**. We measure that with a conservative 95% statistical check. Weâ€™ll also look at performance separately for **internal vs. external** descriptions and across **major role groups** (e.g., Specialist, Analyst, Director). We only exclude items with an empty/too-short Position Summary; other missing fields are okay because the model (and human reviewer) can infer whatâ€™s needed.\n",
        "\n",
        "  If we find recurring miss-patterns (like confusing seniority or over-weighting job titles vs. duties), weâ€™ll **tune the prompt** and rerun. The notebook produces a clean adjudication sheet for the human reviewer, calculates accuracy with confidence intervals, and prints a clear **PASS/REJECT**decision.\n",
        "\n",
        "* **Method Details**: Design. Prospective evaluation of an AI Assistant that classifies job descriptions into an MNPS top-1 job title (primary endpoint) and major/minor role (secondary endpoints).\n",
        "\n",
        "  **Dataset.** We evaluate on a **random sample** of both internal MNPS and external job descriptions from [New Sample_08.07.2025.csv](https://github.com/vanderbilt-data-science/MNPSCollaborative/blob/main/New%20Sample_08.07.2025.csv). The **only** a priori exclusion is records lacking sufficient text in the **Position Summary** field, which is required for adjudication. Missing or sparse content in other fields (e.g., Education, Work Experience, Licenses/Certifications, KSAs) is not exclusionary because those signals can often be inferred from the Position Summary or recovered by the AI model during classification.\n",
        "\n",
        "  **Model and Outputs.** For each description, the Assistant returns a structured JSON with: predicted title, major role, minor role, a 0â€“1 confidence score, a brief rationale, and a prompt version tag. JSON is schema-checked before scoring.\n",
        "\n",
        "    **Ground truth.** A human evaluator reviews each prediction. Where used for formal reporting, we recommend dual independent review with adjudication and reporting **inter-rater reliability** (e.g., Cohenâ€™s Îºâ‰¥0.75), but the protocol supports single-evaluator adjudication for prompt-tuning cycles.\n",
        "\n",
        "  **Primary outcome and acceptance criterion.** Top-1 title accuracy with a two-sided Clopperâ€“Pearson 95% confidence interval. We accept the model if the lower bound â‰¥ 0.90. This rule is pre-specified and applied once per evaluation run.\n",
        "\n",
        "  **Secondary outcomes.** (i) Title accuracy by Source (Internal vs. External) and by Major role group, each with Wilson 95% intervals for readability; (ii) Major/minor correctness rates; (iii) Error taxonomy counts (e.g., â€œseniority misread,â€ â€œwrong job family,â€ â€œduties overweighted/underweightedâ€).\n",
        "\n",
        "  **Analysis plan.** The notebook calculates overall accuracy and confidence intervals, prints a PASS/REJECT decision, and exports subgroup tables and error buckets for prompt iteration. An optional checkpoint table reports the minimum number correct required for the acceptance lower-bound at common sample sizes. Prompt changes are versioned; re-tests are run on the full set after targeted fixes informed by the error taxonomy.\n",
        "\n",
        "  **Bias & limitations.** External job descriptions vary in style and detail; misclassification risk rises when licensure or scope signals are missing. To mitigate, the prompt explicitly weights Essential Functions, Education/Experience, and Licenses/Certifications over job title wording and brand terms. Results generalize to descriptions similar in content and detail to the sample.\n",
        "\n",
        "  **Reproducibility.** The notebook fixes the analysis rule (exact 95% CI lower-bound â‰¥0.90), logs (record_id, prompt_version, model_json, timestamp), exports the human adjudication sheet and scored results, and supports re-runs with updated prompts."
      ],
      "metadata": {
        "id": "Vxr9nUDPhuC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1** | One-Glance Notebook Pipeline\n",
        "0 â†’ 1 â†’ 2.1 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ (Human) â†’ 7 â†’ 8 â†’ 9 â†’ 10 â†’ 11\n",
        "\n",
        "0. Overview: Why and how (plain-English + methods).\n",
        "\n",
        "1. Config & acceptance rule: Set knobs and the PASS rule (exact 95% LB â‰¥ 0.90).\n",
        "\n",
        "2. Configuration and Acceptance Rule\n",
        "\n",
        "  2.1 Eval Sample Load: Load New Sample_08.07.2025.csv, exclude only thin Position Summaries, assign Record_ID.\n",
        "\n",
        "3. The Data: Supporting resources & how to run the model (keep as-is).\n",
        "\n",
        "4. The Prompts: Dev prompt + Pydantic schema + parsing; (keep as-is, with tiny fixes).\n",
        "\n",
        "5. Batch run: Run model for each record in df_eval â†’ model_outputs.csv.\n",
        "\n",
        "6. Adjudication sheet: Merge source text + predictions â†’ MNPS_Adjudication_Sheet.csv.\n",
        "\n",
        "  **(Human Step):** Reviewer fills Y/N + final labels â†’ MNPS_Adjudication_Filled.csv.\n",
        "\n",
        "7. Score & decision: Read filled sheet; compute accuracy + exact CI; PASS/REJECT; Wilson by subgroups; error buckets.\n",
        "\n",
        "8. Subgroups & diagnostics: Breakouts (Internal/External, Major), top error categories (prompt fixes).\n",
        "\n",
        "9. Helper (min successes): Quick table of â€œmin correct to reach LBâ‰¥0.90â€ for common n.\n",
        "\n",
        "10. Exports: Save subgroup tables and error buckets for reporting.\n",
        "\n",
        "11. Next actions: If REJECT, patch prompt (targeted), bump version, re-run (first failures, then full)."
      ],
      "metadata": {
        "id": "eZrntiEO0ZB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2** | Notebook Section Breakdown and Continuity\n",
        "**0. Overview (Markdown)**\n",
        "  \n",
        "  * Purpose: Narrative that frames the evaluation; what â€œPASSâ€ means.\n",
        "  * Inputs: None.\n",
        "  * Outputs: Documentation only.\n",
        "  * Who: You (authoring).\n",
        "\n",
        "**1. Config and Acceptance rule (Code)**\n",
        "\n",
        "  * Purpose: Central place for all constants (e.g., ACCEPT_LB=0.90, ALPHA=0.05, file paths).\n",
        "  * Inputs: None (you set values).\n",
        "  * Outputs: Globals used downstream (ACCEPT_LB, MIN_SUMMARY_CHARS, filenames).\n",
        "  * Who: You (tuning).\n",
        "\n",
        "**2. Evaluation Sample Load (Code)**\n",
        "    \n",
        "  * Purpose: Create a clean evaluation table.\n",
        "  *Inputs: New Sample_08.07.2025.csv.\n",
        "  *Logic: Only exclude rows with empty/too-short Position Summary; keep others even if sparse.\n",
        "  *Outputs: df_eval with a stable Record_ID.\n",
        "  *Who: Code (automated).\n",
        "\n",
        "**3. The Data (Keep as-is)**\n",
        "\n",
        "  * Purpose: Any resources/instructions needed to run the actual model (KSACs, roles, etc.).\n",
        "  * Inputs: Reference files you use in prompts/logic.\n",
        "  * Outputs: Context/material for Section 4.\n",
        "  * Who: You & readers (instructions), model (as needed).\n",
        "\n",
        "**4. The Prompts (Keep as-is; add tiny fixes if needed)**\n",
        "\n",
        "  * Purpose: Developer prompt, Pydantic schema, OpenAI call, structured parse.\n",
        "  * Inputs: One recordâ€™s text (from df_eval) per call; your prompt.\n",
        "  * Outputs: Structured prediction(s) (title/major/minor/confidence/rationale), parsable by code.\n",
        "  * Who: Model + your code.\n",
        "\n",
        "  * Tiny fixes (where to place):\n",
        "  â€¢ Add from typing import List, Optional right under your Pydantic import.\n",
        "  â€¢ Replace any dict(*parsed.job_classification_table) with [item.model_dump() for item in parsed.job_classification_table].\n",
        "\n",
        "**5. Batch model run (Code)**\n",
        "\n",
        "  * Purpose: Loop over all df_eval rows, call Section-4 model code, collect outputs.\n",
        "  * Inputs: df_eval, prompt/schema from Section 4.\n",
        "  * Outputs: results_df (in-memory) and model_outputs.csv on disk.\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**6. Build adjudication sheet (Code)**\n",
        "\n",
        "  * Purpose: Create the file the human will grade.\n",
        "  * Inputs: df_eval (source text) + results_df (predictions).\n",
        "  * Outputs: MNPS_Adjudication_Sheet.csv with blank Y/N and final label fields.\n",
        "  * Who: Code produces; Human fills.\n",
        "\n",
        "**(Human) Adjudication**\n",
        "\n",
        "  * Purpose: Gold standard decision per row.\n",
        "  * Inputs: MNPS_Adjudication_Sheet.csv.\n",
        "  * Outputs: MNPS_Adjudication_Filled.csv (same columns, Y/N + final labels completed).\n",
        "  * Who: Human reviewer(s).\n",
        "\n",
        "**7. Ingest Adjudication and Score (Code)**\n",
        "\n",
        "  * Purpose: Apply the pre-specified acceptance rule and show diagnostics\n",
        "  * Inputs: MNPS_Adjudication_Filled.csv.\n",
        "  * Calculations:\n",
        "      **Primary:** top-1 title accuracy with exact two-sided 95% CI; PASS if LB â‰¥ 0.90.\n",
        "      **Secondary:** Wilson CIs by Source and (if present) Major; error categories.\n",
        "  * Outputs: Console decision + summary; in-memory tables (by_src, by_major, err_counts).\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**8. Subgroups and Diagnostics (Code)**\n",
        "\n",
        "  * Purpose: Breakouts and error taxonomy for targeted prompt fixes.\n",
        "  * Inputs: Adjudicated table from 7.\n",
        "  * Outputs: Summaries for reporting/iteration (and later exported in 10).\n",
        "  * Who: Code (automated), You (interpretation).\n",
        "\n",
        "**9. Helper: min k for LBâ‰¥target (Code)**\n",
        "\n",
        "  * Purpose: Quick reference of â€œhow many correct do we need at size n?â€\n",
        "  * Inputs: Target LB, alpha.\n",
        "  * Outputs: Printed thresholds for common n (100, 200, 300, â€¦).\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**10. Export Tables (Code)**\n",
        "\n",
        "  * Purpose: Persist the subgroup and error summaries.\n",
        "  * Inputs: Tables from 7â€“8.\n",
        "  * Outputs: results_by_source.csv, results_by_major.csv (if present), results_error_buckets.csv.\n",
        "  * Who: Code (automated).\n",
        "\n",
        "**11. Next Actions (Markdown)**\n",
        "\n",
        "  * Purpose: The iteration plan if REJECT (or what to lock if PASS).\n",
        "  * Inputs: Diagnostics from 7â€“8.\n",
        "  * Outputs: To-dos: patch prompt, bump PROMPT_VERSION, re-run fails â†’ full set, re-score.\n",
        "  * Who: You (decision + edits).\n",
        "\n"
      ],
      "metadata": {
        "id": "K1dyMtnvJEbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3** | Run Order Checklist\n",
        "1. Run Sections 1 â†’ 2.1 to load & gate the sample.\n",
        "2. (Optional) glance at 3 for resources.\n",
        "3. Confirm 4 runs cleanly on one row (schema + parse).\n",
        "4. Run 5 to batch predictions â†’ model_outputs.csv.\n",
        "5. Run 6 to create MNPS_Adjudication_Sheet.csv; send to reviewer.\n",
        "6. After review, place MNPS_Adjudication_Filled.csv next to the notebook.\n",
        "7. Run 7 (decision) + 8â€“10 (diagnostics & exports).\n",
        "8. If REJECT, follow 11 (patch prompt, bump version, re-run).\n",
        "\n"
      ],
      "metadata": {
        "id": "tfdWVEKkP_iF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4** | Artifact Map (what files get created)\n",
        "* model_outputs.csv â†’ Section 5 (raw model predictions).\n",
        "* MNPS_Adjudication_Sheet.csv â†’ Section 6 (for the human).\n",
        "* MNPS_Adjudication_Filled.csv â†’ Human returns this (input to Section 7).\n",
        "* results_by_source.csv, results_by_major.csv, results_error_buckets.csv â†’ Section 10 (reporting)."
      ],
      "metadata": {
        "id": "3sBw0As6Qfli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2** | Configuration and Acceptance Rule\n",
        "Again, you're completely free to just download this notebook, create a local virtual environment and get to coding in your favorite IDE. We provide this code just as a rapid method to get started, and focus our efforts on implementation through Google Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "zYYyUWf7ltd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "BMAhEZJdq3y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "\n",
        "# set OpenAI API key environment variable using Google Colab\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "q9RrxxQNPb-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIG ---\n",
        "ACCEPT_LB = 0.90   # lower bound target\n",
        "ALPHA = 0.05       # 95% confidence\n",
        "PRIMARY_FIELD = \"title\"  # primary metric: top-1 title accuracy\n",
        "\n",
        "# Data location: use ONE of the following\n",
        "DATA_URL = \"https://raw.githubusercontent.com/vanderbilt-data-science/MNPSCollaborative/main/New%20Sample_08.07.2025.csv\"\n",
        "DATA_PATH = None   # e.g., \"./New Sample_08.07.2025.csv\" if local\n",
        "\n",
        "# Required input columns (from the CSV headers provided)\n",
        "REQUIRED_COLS = [\n",
        "    \"Job Description Name\",\n",
        "    \"Position Summary\",\n",
        "    \"Education\",\n",
        "    \"Work Experience\",\n",
        "    \"Essential Functions\",\n",
        "    \"Licenses and Certifications\",\n",
        "    \"Knowledge, Skills and Abilities\",\n",
        "    \"Source\"   # Internal / External\n",
        "]\n",
        "\n",
        "# Allowed labels for model outputs (extend if needed)\n",
        "ALLOWED_MAJORS = [\n",
        "    \"Specialist\", \"Analyst\", \"Director\", \"Manager\", \"Technician\", \"Coordinator\"\n",
        "]\n",
        "\n",
        "# Output filenames\n",
        "MODEL_OUT_CSV = \"model_outputs.csv\"\n",
        "ADJ_SHEET_CSV = \"MNPS_Adjudication_Sheet.csv\"  # to be filled by human\n",
        "ADJ_FILLED_CSV = \"MNPS_Adjudication_Filled.csv\"  # humanâ€‘completed file name (you will upload/point to it later)"
      ],
      "metadata": {
        "id": "oxFiwmBMbaMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1** | Evaluation Sample Load and Only-Exclusion Rule\n",
        "This section is the s the â€œload and gatekeeperâ€ for the evaluation set.\n",
        "\n",
        "* It loads the sample CSV (New Sample_08.07.2025.csv) from the repo (or a local path if you switch DATA_PATH).\n",
        "* Enforces the only exclusion rule: drops rows whose Position Summary is empty/too short (â‰¤ MIN_SUMMARY_CHARS).\n",
        "* Normalizes (trims) the Position Summary text.\n",
        "* Creates a stable ID (Record_ID) if the file doesnâ€™t already have one.\n",
        "* Hands off a clean table called df_eval for the later â€œrun model â†’ adjudicate â†’ scoreâ€ steps."
      ],
      "metadata": {
        "id": "MIyRvtRisuCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EVALUATION SAMPLE ---\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/vanderbilt-data-science/MNPSCollaborative/main/New%20Sample_08.07.2025.csv\"\n",
        "MIN_SUMMARY_CHARS = 10  # only a priori exclusion\n",
        "\n",
        "df_eval = pd.read_csv(DATA_URL)\n",
        "if \"Position Summary\" not in df_eval.columns:\n",
        "    raise ValueError(\"Missing required 'Position Summary' column.\")\n",
        "\n",
        "# Only exclusion: thin/empty Position Summary\n",
        "df_eval[\"Position Summary\"] = df_eval[\"Position Summary\"].astype(str).str.strip()\n",
        "df_eval = df_eval[df_eval[\"Position Summary\"].str.len() > MIN_SUMMARY_CHARS].copy()\n",
        "\n",
        "# Stable ID for joining later\n",
        "if \"Record_ID\" not in df_eval.columns:\n",
        "    df_eval.insert(0, \"Record_ID\", np.arange(1, len(df_eval)+1))\n",
        "\n",
        "print(f\"Evaluation rows loaded: n={len(df_eval)}\")\n",
        "df_eval.head(3)"
      ],
      "metadata": {
        "id": "ogkMGAvJtCzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3** | The Data\n",
        "\n",
        "The current prompt is a two-step prompt that is successful through the ChatGPT interface. It requires two types of data:\n",
        "* The data to be classified\n",
        "* Supporting resources\n",
        "\n",
        "We need to read all of this in. Let's grab it and use it. The first thing you'll do is just straight up download a zip file of all of this information.\n",
        "\n",
        "You can download all of the reference files from the link provided, then upload in the sidebar. You'll then unzip the directory using the code below.\n",
        "\n",
        "Click on the folder icon in the left sidebar (kinda looks like this ðŸ—‚ï¸) and you'll see all the files there. We'll read them in.\n"
      ],
      "metadata": {
        "id": "vdVEU9lGlnC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/2025u-mnps-minihackathon.zip"
      ],
      "metadata": {
        "id": "iaKoFAightj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resources_dir_prefix = '/content/2025u-mnps-minihackathon/prompt-resources/'\n",
        "roles_lookup = pd.read_csv(resources_dir_prefix+\"MNPS Roles.csv\")\n",
        "determinants = pd.read_csv(resources_dir_prefix+\"Competency Extended Descriptions.csv\", encoding='latin1')\n",
        "ksac_table = pd.read_csv(resources_dir_prefix+\"MNPS KSACs.csv\")\n",
        "korn_ferry = pd.read_csv(resources_dir_prefix+\"Korn_Ferry Lominger 38 Competencies.csv\", encoding='latin1')"
      ],
      "metadata": {
        "id": "xn87fRq-ViIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4** | The Prompts\n",
        "\n",
        "What we have here is a direct prompt to get the response that we're looking for. We'll make this happen directly using the OpenAI Chat Completions API. Note that you can use other APIs as you like."
      ],
      "metadata": {
        "id": "m0bzttINsCQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4stx_ljahbqt"
      },
      "outputs": [],
      "source": [
        "zero_shot_prompt = \\\n",
        "\"\"\" Objective: Evaluate and group jobs from the \"Job Description Export Specialists.xlsx\" file based on similarities in job functions, not job titles.\n",
        "\n",
        "Process:\n",
        "\n",
        "- Compare all jobs against each other using the attributes listed in the file: Education, Work Experience, Licenses/Certifications, Essential Functions, Knowledge, Skills, Abilities, and Position Summary.\n",
        "- Compare each job with reference sources using the same attributes. I have attached the reference sources for you.\n",
        "- Group jobs based on similarities into:\n",
        "  - Major role groupings (e.g., Specialist, Analyst, Manager)\n",
        "  - Minor sub-groupings (e.g., Specialist I, II, III, IV) - not to exceed level IV\n",
        "- Use the MNPS Roles and MNPS KSACs documents to help you determine major role groupings.\n",
        "- Use the remaining documents to help you clarify subtle differences in role groupings and sub-groupings.\n",
        "- Use a more qualitative, holistic assessment focused on functional alignment with KSACs rather than a quantitative scoring approach with defined complexity metrics\n",
        "\n",
        "Output Format:\n",
        "\n",
        "- Create a table with the following columns:\n",
        "  - Original Job Title\n",
        "  - New Job Title\n",
        "  - Major Role Group\n",
        "  - Minor Sub-Group\n",
        "  - Justification for Grouping\n",
        "\n",
        "- Provide an accompanying narrative explaining the rationale behind the groupings and any notable patterns or insights discovered during the analysis.\n",
        "\n",
        "Job Title Convention:\n",
        "\n",
        "- Follow the format: \"[Function] [Role] [Level]\" (e.g., \"Collections Specialist II\", \"Accounts Payable Specialist III\")\n",
        "\n",
        "Additional Guidelines:\n",
        "\n",
        "- Ensure all sources used are cited properly.\n",
        "- Focus on the nature of the work performed rather than just the job titles.\n",
        "- Consider the complexity of tasks, level of responsibility, and required competencies when determining groupings.\n",
        "- Provide clear explanations for why each job was classified as it was, referencing specific job attributes and external benchmarks.\n",
        "\n",
        "CONSTRAINTS:\n",
        "1) Prioritize signals in this order: Essential Functions > Education/Experience > Licensure/Certifications. Ignore employer branding and title fluff unless supported by duties/scope.\n",
        "2) Restrict major_role_group to one of: Specialist, Analyst, Director, Manager, Technician, Coordinator.\n",
        "3) Always return confidence_0to1 as a conservative float in [0, 1].\n",
        "4) Echo back the record_id provided in the input.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of asking for a table output, we will use **structured outputs**. Though this is a common approach for the outputs of LLMs/AI systems, you can learn more about this on [OpenAI's structured output documentation](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses). Note that you can find this information on almost all LLM/AI platform or package providers."
      ],
      "metadata": {
        "id": "v2fP0nG0NqB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional  # â† add this line\n",
        "\n",
        "class JobClassification(BaseModel):\n",
        "    \"\"\"Represents the classification of a single job.\"\"\"\n",
        "    record_id: int = Field(..., description=\"Record_ID from the evaluation CSV.\")\n",
        "    job_title_original: str = Field(..., description=\"Original job title from input.\")\n",
        "    new_job_title: str = Field(..., description=\"Proposed MNPS-style title.\")\n",
        "    major_role_group: str = Field(..., description=\"One of: Specialist, Analyst, Director, Manager, Technician, Coordinator.\")\n",
        "    minor_sub_group: str = Field(..., description=\"Level (e.g., I, II, III, IV) or blank if not applicable.\")\n",
        "    grouping_justification: str = Field(..., description=\"One-sentence rationale anchored on Essential Functions, Education/Experience, and Licensure.\")\n",
        "    confidence_0to1: float = Field(..., ge=0.0, le=1.0, description=\"Self-rated confidence, conservative calibration.\")\n",
        "\n",
        "class JobClassificationTable(BaseModel):\n",
        "    job_classification_table: List[JobClassification] = Field(..., description=\"One entry per input job.\")\n",
        "    narrative_rationale: str = Field(..., description=\"Optional narrative across the set.\")\n",
        ""
      ],
      "metadata": {
        "id": "cs3EhFP4OLFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create classifications using OpenAI. Of note here is:\n",
        "* The **developer** prompt - this is the \"system prompt\" or \"custom instructions\" for the model. This determines the overall behavior of the model.\n",
        "* The **user** prompt - this is what we send to the model like when we're chatting with ChatGPT."
      ],
      "metadata": {
        "id": "oRPxg9JEPDt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create openAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Create messages to send\n",
        "messages = [\n",
        "    {\"role\": \"developer\", \"content\": zero_shot_prompt},\n",
        "    {\"role\": \"user\", \"content\": \"Classify the following job description: [Paste Job Description Here]\"} # Replace with actual job description\n",
        "]\n",
        "\n",
        "# Assuming JobClassification and zero_shot_prompt are defined in the preceding code\n",
        "response = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o\", # Or another available model\n",
        "    messages=messages,\n",
        "    temperature=0.2, #changed from 1\n",
        "    max_tokens=1000,\n",
        "    response_format=JobClassificationTable,   # â† this\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ],
      "metadata": {
        "id": "E9ff_JExPDMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at response\n",
        "response.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "cAzjnoIeQdBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make this into a table using pandas!"
      ],
      "metadata": {
        "id": "DnuQIiz9c_Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the structured result (optional)\n",
        "parsed = response.choices[0].message.parsed\n",
        "\n",
        "# Extract the table (list of JobClassification objects)\n",
        "table = parsed.job_classification_table\n",
        "\n",
        "# If you expect exactly one classification, single-row dict:\n",
        "first_item = table[0].model_dump()     # .model_dump() for Pydantic v2\n",
        "# first_item -> {'job_title_original': ..., 'new_job_title': ..., ...}\n",
        "\n",
        "# Or, if you want a DataFrame of all rows:\n",
        "rows = [item.model_dump() for item in table]\n",
        "results_df = pd.DataFrame(rows)\n",
        "results_df.head()"
      ],
      "metadata": {
        "id": "1NoOUqyocxu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5** | Batch Model Run on the Evaluation Sample\n",
        "\n",
        "Runs the AI once per row in df_eval (your filtered New Sample_08.07.2025.csv), using your Section-4 prompt/schema.\n",
        "* **Inputs:** df_eval, your zero_shot_prompt, model name, and PROMPT_VERSION.\n",
        "* **Outputs:** A tidy predictions table with one row per record:\n",
        "\n",
        "    * Record_ID, Model_Prompt_Version, Model_Run_Timestamp\n",
        "\n",
        "    * Model_Pred_Title, Model_Pred_Major, Model_Pred_Minor\n",
        "\n",
        "    * Model_Confidence_0to1, Model_Rationale_Short\n",
        "It saves this as model_outputs.csv. (No scoring yetâ€”just model outputs.)"
      ],
      "metadata": {
        "id": "IkgwG4FrvxF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, datetime as dt\n",
        "from openai import OpenAI\n",
        "\n",
        "PROMPT_VERSION = \"v1\"   # bump when you change the prompt\n",
        "MODEL_NAME = \"gpt-4o\"   # or your preferred model\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def classify_one(row):\n",
        "    user_payload = f\"\"\"\n",
        "Record_ID: {int(row['Record_ID'])}\n",
        "Original Job Title: {row.get('Job Description Name','')}\n",
        "Position Summary: {row.get('Position Summary','')}\n",
        "Education: {row.get('Education','')}\n",
        "Work Experience: {row.get('Work Experience','')}\n",
        "Licenses and Certifications: {row.get('Licenses and Certifications','')}\n",
        "Knowledge, Skills and Abilities: {row.get('Knowledge, Skills and Abilities','')}\n",
        "\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"developer\", \"content\": zero_shot_prompt},\n",
        "        {\"role\": \"user\", \"content\": \"Classify the following job description. Output 1 item in job_classification_table.\\n\" + user_payload}\n",
        "    ]\n",
        "    resp = client.beta.chat.completions.parse(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        temperature=0.2,             # tighter, more deterministic\n",
        "        max_tokens=1000,\n",
        "        response_format=JobClassificationTable,\n",
        "    )\n",
        "    parsed = resp.choices[0].message.parsed.job_classification_table[0].model_dump()\n",
        "    # Normalize to our evaluation columns\n",
        "    return {\n",
        "        \"Record_ID\": parsed[\"record_id\"],\n",
        "        \"Model_Prompt_Version\": PROMPT_VERSION,\n",
        "        \"Model_Run_Timestamp\": dt.datetime.utcnow().isoformat(),\n",
        "        \"Model_Pred_Title\": parsed[\"new_job_title\"],\n",
        "        \"Model_Pred_Major\": parsed[\"major_role_group\"],\n",
        "        \"Model_Pred_Minor\": parsed[\"minor_sub_group\"],\n",
        "        \"Model_Confidence_0to1\": float(parsed[\"confidence_0to1\"]),\n",
        "        \"Model_Rationale_Short\": parsed[\"grouping_justification\"],\n",
        "    }\n",
        "\n",
        "pred_rows = [classify_one(r) for _, r in df_eval.iterrows()]\n",
        "results_df = pd.DataFrame(pred_rows)\n",
        "results_df.to_csv(\"model_outputs.csv\", index=False)\n",
        "print(\"Saved model_outputs.csv\")\n",
        "results_df.head(3)\n"
      ],
      "metadata": {
        "id": "zfQ3cgVOv9vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1** | Bridge Outputs â†’ Standard Columns\n",
        "This cell normalizes what the batch run produced into the columns downstream cells expect. It supports two sources:\n",
        "* a parsed `response` still in memory\n",
        "* an existing DataFrame named `results_df` already built in Section 4/5"
      ],
      "metadata": {
        "id": "ffre-O_eUqPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 â€” Bridge outputs â†’ standard columns\n",
        "import pandas as pd, numpy as np, datetime as dt\n",
        "\n",
        "# This cell normalizes whatever Section 4 (or your batch run) produced into\n",
        "# the columns downstream cells expect. It supports two sources:\n",
        "#   (A) a parsed `response` still in memory\n",
        "#   (B) an existing DataFrame named `results_df` you already built in Section 4/5\n",
        "\n",
        "def _to_rows_from_parsed(parsed_table):\n",
        "    # parsed_table: list of Pydantic objects\n",
        "    rows = []\n",
        "    for item in parsed_table:\n",
        "        if hasattr(item, \"model_dump\"):\n",
        "            rows.append(item.model_dump())  # Pydantic v2\n",
        "        else:\n",
        "            # Fallback if you somehow have dict-like items already\n",
        "            rows.append(dict(item))\n",
        "    return rows\n",
        "\n",
        "# Try (A): pull from a `response` variable left by Section 4\n",
        "_rows = None\n",
        "try:\n",
        "    _parsed = response.choices[0].message.parsed\n",
        "    _rows = _to_rows_from_parsed(_parsed.job_classification_table)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Otherwise (B): use an existing DataFrame `results_df` (if you already built it)\n",
        "if _rows is not None:\n",
        "    _raw_df = pd.DataFrame(_rows)\n",
        "elif \"results_df\" in globals() and isinstance(results_df, pd.DataFrame):\n",
        "    _raw_df = results_df.copy()\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        \"B1 bridge: couldn't find Section-4/5 outputs.\\n\"\n",
        "        \"Ensure you either keep `response` in memory (after parse) or have a DataFrame named `results_df`.\"\n",
        "    )\n",
        "\n",
        "# Map Section-4/5 columns â†’ standardized evaluation columns\n",
        "_cands = {\n",
        "    \"Record_ID\":             [\"record_id\"],  # if absent, we'll backfill on merge with df_eval\n",
        "    \"Model_Pred_Title\":      [\"new_job_title\", \"pred_title\"],\n",
        "    \"Model_Pred_Major\":      [\"major_role_group\", \"pred_major\"],\n",
        "    \"Model_Pred_Minor\":      [\"minor_sub_group\", \"pred_minor\"],\n",
        "    \"Model_Rationale_Short\": [\"grouping_justification\", \"rationale\"],\n",
        "    \"Model_Confidence_0to1\": [\"confidence_0to1\", \"confidence\", \"score\"],\n",
        "    \"Job_Title_Original\":    [\"job_title_original\", \"original_title\"],\n",
        "}\n",
        "\n",
        "def _pick(df, options, default=np.nan):\n",
        "    for col in options:\n",
        "        if col in df.columns:\n",
        "            return df[col]\n",
        "    return default\n",
        "\n",
        "# If you track prompt version elsewhere, set it here\n",
        "PROMPT_VERSION = globals().get(\"PROMPT_VERSION\", \"v1\")\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"Record_ID\":             _pick(_raw_df, _cands[\"Record_ID\"]),\n",
        "    \"Model_Prompt_Version\":  PROMPT_VERSION,\n",
        "    \"Model_Run_Timestamp\":   dt.datetime.utcnow().isoformat(),\n",
        "    \"Model_Pred_Title\":      _pick(_raw_df, _cands[\"Model_Pred_Title\"]),\n",
        "    \"Model_Pred_Major\":      _pick(_raw_df, _cands[\"Model_Pred_Major\"]),\n",
        "    \"Model_Pred_Minor\":      _pick(_raw_df, _cands[\"Model_Pred_Minor\"]),\n",
        "    \"Model_Confidence_0to1\": _pick(_raw_df, _cands[\"Model_Confidence_0to1\"]),\n",
        "    \"Model_Rationale_Short\": _pick(_raw_df, _cands[\"Model_Rationale_Short\"]),\n",
        "    \"Job_Title_Original\":    _pick(_raw_df, _cands[\"Job_Title_Original\"]),\n",
        "})\n",
        "\n",
        "# Persist for downstream cells (optional but convenient)\n",
        "results_df.to_csv(\"model_outputs.csv\", index=False)\n",
        "print(\"Normalized Section-4/5 outputs â†’ results_df (and wrote model_outputs.csv). Preview:\")\n",
        "results_df.head(3)\n"
      ],
      "metadata": {
        "id": "5jatPQdGVSud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6** | Build Adjudication Sheet for the Human\n",
        "\n",
        "Joins the **source text** (Position Summary, etc.) with the **model outputs** so a human can mark correctness.\n",
        "* **Inputs:** df_eval + results_df from Step 5.\n",
        "* **Outputs:** MNPS_Adjudication_Sheet.csv with:\n",
        "\n",
        "  * Source columns (Record_ID, Source, Job Description Name, Position Summary, Education, Work Experience, Essential Functions, Licenses, KSAs)\n",
        "\n",
        "  * Model columns from Step 5\n",
        "\n",
        "  * Empty columns for the reviewer to fill:\n",
        "Adj_Correct_Title (Y/N), Adj_Correct_Major (Y/N), Adj_Correct_Minor (Y/N), Adj_Final_Title, Adj_Final_Major, Adj_Final_Minor, Adj_Error_Category, Adj_Notes\n",
        "This is the single artifact you hand to the adjudicator."
      ],
      "metadata": {
        "id": "QgO6T2R0wB3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_cols = [\n",
        "    \"Record_ID\",\"Source\",\"Job Description Name\",\"Position Summary\",\"Education\",\n",
        "    \"Work Experience\",\"Essential Functions\",\"Licenses and Certifications\",\n",
        "    \"Knowledge, Skills and Abilities\"\n",
        "]\n",
        "for c in base_cols:\n",
        "    if c not in df_eval.columns:\n",
        "        df_eval[c] = np.nan\n",
        "\n",
        "adj = df_eval[base_cols].merge(results_df, on=\"Record_ID\", how=\"left\")\n",
        "\n",
        "# Add empty adjudication fields\n",
        "for c in [\n",
        "    \"Adj_Correct_Title (Y/N)\",\"Adj_Correct_Major (Y/N)\",\"Adj_Correct_Minor (Y/N)\",\n",
        "    \"Adj_Final_Title\",\"Adj_Final_Major\",\"Adj_Final_Minor\",\n",
        "    \"Adj_Error_Category\",\"Adj_Notes\"\n",
        "]:\n",
        "    if c not in adj.columns:\n",
        "        adj[c] = \"\"\n",
        "\n",
        "adj.to_csv(\"MNPS_Adjudication_Sheet.csv\", index=False)\n",
        "print(\"Wrote MNPS_Adjudication_Sheet.csv â€” share with the reviewer.\")\n",
        "adj.head(2)\n"
      ],
      "metadata": {
        "id": "VM5LM8xFwRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7** | Ingest Adjudication and Compute PASS/REJECT\n",
        "\n",
        "Reads the reviewerâ€™s completed sheet and applies the acceptance rule.\n",
        "* **Inputs:** MNPS_Adjudication_Filled.csv (same columns as the sheet from Step 6, now filled in).\n",
        "  * **Process:**\n",
        "\n",
        "    * Normalizes Y/N â†’ booleans; if Y/N missing, falls back to comparing Model_* vs Adj_Final_*.\n",
        "\n",
        "    * Computes top-1 title accuracy and its two-sided Clopperâ€“Pearson 95% CI.\n",
        "\n",
        "    * Decision: **PASS** if the exact 95% lower bound â‰¥ 0.90, else **REJECT.**\n",
        "\n",
        "    * **Diagnostics:**\n",
        "\n",
        "      * **By Source** (Internal/External) with Wilson 95% CIs.\n",
        "\n",
        "      * **Top Error Categories** (from Adj_Error_Category) to guide prompt fixes.\n",
        "* **Outputs:** Console printout with n, correct_titles, accuracy, exact/Wilson CIs, and the PASS/REJECT decision; plus subgroup and error-bucket summaries. (You can export these to CSV if you like; the earlier â€œexportâ€ cell handles that.)\n"
      ],
      "metadata": {
        "id": "lVsDl6zDwXGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from scipy.stats import beta\n",
        "\n",
        "ALPHA = 0.05\n",
        "ACCEPT_LB = 0.90\n",
        "\n",
        "adj_filled = pd.read_csv(\"MNPS_Adjudication_Filled.csv\")\n",
        "\n",
        "yn = {\"y\": True, \"yes\": True, \"n\": False, \"no\": False, \"\": np.nan}\n",
        "for col in [\"Adj_Correct_Title (Y/N)\",\"Adj_Correct_Major (Y/N)\",\"Adj_Correct_Minor (Y/N)\"]:\n",
        "    adj_filled[col] = adj_filled[col].astype(str).str.strip().str.lower().map(yn)\n",
        "\n",
        "def fallback(eq_col, model_col, final_col):\n",
        "    return adj_filled[eq_col].where(adj_filled[eq_col].notna(),\n",
        "           (adj_filled[model_col].fillna(\"\").astype(str).str.strip() ==\n",
        "            adj_filled[final_col].fillna(\"\").astype(str).str.strip()))\n",
        "\n",
        "adj_filled[\"is_correct_title\"] = fallback(\"Adj_Correct_Title (Y/N)\",\"Model_Pred_Title\",\"Adj_Final_Title\")\n",
        "\n",
        "def wilson_ci(k, n):\n",
        "    if n == 0: return (np.nan, np.nan)\n",
        "    z = 1.959963984540054\n",
        "    p = k/n\n",
        "    denom = 1 + (z**2)/n\n",
        "    centre = p + (z**2)/(2*n)\n",
        "    adj = z * math.sqrt( (p*(1-p) + (z**2)/(4*n)) / n )\n",
        "    lb = (centre - adj)/denom\n",
        "    ub = (centre + adj)/denom\n",
        "    return max(0.0, lb), min(1.0, ub)\n",
        "\n",
        "def clopper_pearson_ci(k, n, alpha=ALPHA):\n",
        "    if n == 0: return (np.nan, np.nan)\n",
        "    a = alpha/2\n",
        "    lb = 0.0 if k == 0 else beta.ppf(a, k, n-k+1)\n",
        "    ub = 1.0 if k == n else beta.ppf(1-a, k+1, n-k)\n",
        "    return float(lb), float(ub)\n",
        "\n",
        "n = len(adj_filled)\n",
        "k = int(adj_filled[\"is_correct_title\"].sum())\n",
        "acc = k/n if n else np.nan\n",
        "lb, ub = clopper_pearson_ci(k, n, ALPHA)\n",
        "lb_w, ub_w = wilson_ci(k, n)\n",
        "\n",
        "print({\n",
        "    \"n\": n,\n",
        "    \"correct_titles\": k,\n",
        "    \"accuracy\": round(acc,4),\n",
        "    \"exact95_CI\": (round(lb,4), round(ub,4)),\n",
        "    \"wilson95_CI\": (round(lb_w,4), round(ub_w,4)),\n",
        "    \"accept_rule\": f\"PASS if exact95 lower bound â‰¥ {ACCEPT_LB:.2f}\",\n",
        "    \"DECISION\": \"PASS\" if lb >= ACCEPT_LB else \"REJECT\"\n",
        "})\n",
        "\n",
        "# Subgroups & error buckets\n",
        "by_src = adj_filled.groupby(\"Source\", dropna=False)[\"is_correct_title\"].agg([\"count\",\"sum\"])\n",
        "by_src[\"accuracy\"] = by_src[\"sum\"] / by_src[\"count\"]\n",
        "by_src[\"wilson_lb\"], by_src[\"wilson_ub\"] = zip(*[\n",
        "    wilson_ci(int(r[\"sum\"]), int(r[\"count\"])) for _, r in by_src.iterrows()\n",
        "])\n",
        "print(\"\\nBy Source:\\n\", by_src)\n",
        "\n",
        "err = adj_filled[adj_filled[\"is_correct_title\"] == False]\n",
        "print(\"\\nTop Error Categories:\\n\", err[\"Adj_Error_Category\"].value_counts(dropna=False).head(15))\n"
      ],
      "metadata": {
        "id": "mBN3WZf6wfKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}