{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/MNPSCollaborative/blob/Bug-Fixes-Using-Local-File-Load-and-Smoke-Tests/MNPS_Eval_Practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "799338ec",
      "metadata": {
        "id": "799338ec"
      },
      "source": [
        "\n",
        "# MNPS Practical Evaluation Notebook (Data‑Science Style)\n",
        "\n",
        "**Purpose:** Evaluate model outputs with simple accuracy gates and cost‑of‑error analysis instead of statistical inference.\n",
        "\n",
        "- **Keep Section 4 as-is** (your batch inference that writes predictions to CSV).\n",
        "- **Revised Sections 1–3**: configuration and helpers for the practical evaluation.\n",
        "- **New Sections 5–7**: error thresholds and cost‑of‑error analysis for Major/Minor role groupings.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d95f9589",
      "metadata": {
        "id": "d95f9589"
      },
      "source": [
        "\n",
        "## 1) Overview & Acceptance Gates (Engineering Style)\n",
        "\n",
        "We use clear, operational acceptance gates based on what an experienced HR job classification specialist would consider acceptable:\n",
        "\n",
        "- **Major Role Grouping** acceptable error rate: **≤ 2%** (e.g., Analyst vs. Manager misgroup).\n",
        "- **Minor Role Grouping** acceptable error rate: **≤ 5%** (e.g., I vs II vs III vs Lead).\n",
        "\n",
        "We also quantify **severity** via a **cost‑of‑error** analysis:\n",
        "- For **Major** roles, the cost of an error is the **absolute difference** between the annual values mapped to the *true* vs the *predicted* group.\n",
        "- For **Minor** roles, the cost of an error is the **absolute difference** between the values mapped to the *true* vs the *predicted* level.\n",
        "\n",
        "> Rationale: Misclassifying across groups/levels can create pay, equity, and staffing impacts. Using the pay deltas captures the seriousness of the error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949b2b6a",
      "metadata": {
        "id": "949b2b6a"
      },
      "source": [
        "\n",
        "## 2) Config & File Paths\n",
        "\n",
        "Set your file paths and constants here. **Do not** change class labels unless your data uses a different spelling; these are the canonical labels used in this evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==== Mount Google Drive =======\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ycPXdiSzvlbP"
      },
      "id": "ycPXdiSzvlbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121f5b94",
      "metadata": {
        "id": "121f5b94"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==== Required file paths (EDIT as needed) ====\n",
        "#PREDICTIONS_CSV = \"data/batch_predictions.csv\"  # produced by Section 4 (unchanged)\n",
        "\n",
        "# ==== Class label vocabularies ====\n",
        "MAJOR_CLASSES = [\"Technician\", \"Specialist\", \"Analyst\", \"Manager\", \"Coordinator\", \"Director\", \"Other\"]\n",
        "MINOR_CLASSES = [\"I\", \"II\", \"III\", \"Lead\"]\n",
        "\n",
        "# ==== Acceptance gates ====\n",
        "ACCEPTABLE_MAJOR_ERROR_RATE = 0.02  # ≤ 2% errors\n",
        "ACCEPTABLE_MINOR_ERROR_RATE = 0.05  # ≤ 5% errors\n",
        "\n",
        "# ==== Cost maps (annual $) ====\n",
        "COST_MAP_MAJOR = {\n",
        "    \"Technician\": 54225.00,\n",
        "    \"Specialist\": 68765.00,\n",
        "    \"Analyst\":   78208.00,\n",
        "    \"Manager\":  103904.00,\n",
        "    \"Coordinator\": 123133.00,\n",
        "    \"Director\": 146246.00,\n",
        "    \"Other\":    73780.00,   # If \"Other\" appears, you can adjust as needed.\n",
        "}\n",
        "\n",
        "COST_MAP_MINOR = {\n",
        "    \"I\":    4405.00,\n",
        "    \"II\":   3366.94,\n",
        "    \"III\":  6800.00,\n",
        "    \"Lead\": 6800.00,\n",
        "}\n",
        "\n",
        "# === Severity of Error (7×7 major grid; rows=true, cols=pred) ===\n",
        "SEVERITY_MAJOR_GRID = {\n",
        "    (\"Technician\",\"Technician\"):0, (\"Technician\",\"Specialist\"):4, (\"Technician\",\"Analyst\"):5, (\"Technician\",\"Manager\"):6, (\"Technician\",\"Coordinator\"):4, (\"Technician\",\"Director\"):7, (\"Technician\",\"Other\"):6,\n",
        "    (\"Specialist\",\"Technician\"):4, (\"Specialist\",\"Specialist\"):0, (\"Specialist\",\"Analyst\"):4, (\"Specialist\",\"Manager\"):5, (\"Specialist\",\"Coordinator\"):3, (\"Specialist\",\"Director\"):6, (\"Specialist\",\"Other\"):6,\n",
        "    (\"Analyst\",\"Technician\"):5, (\"Analyst\",\"Specialist\"):4, (\"Analyst\",\"Analyst\"):0, (\"Analyst\",\"Manager\"):4, (\"Analyst\",\"Coordinator\"):4, (\"Analyst\",\"Director\"):5, (\"Analyst\",\"Other\"):6,\n",
        "    (\"Manager\",\"Technician\"):6, (\"Manager\",\"Specialist\"):5, (\"Manager\",\"Analyst\"):4, (\"Manager\",\"Manager\"):0, (\"Manager\",\"Coordinator\"):5, (\"Manager\",\"Director\"):4, (\"Manager\",\"Other\"):6,\n",
        "    (\"Coordinator\",\"Technician\"):4, (\"Coordinator\",\"Specialist\"):3, (\"Coordinator\",\"Analyst\"):4, (\"Coordinator\",\"Manager\"):5, (\"Coordinator\",\"Coordinator\"):0, (\"Coordinator\",\"Director\"):6, (\"Coordinator\",\"Other\"):6,\n",
        "    (\"Director\",\"Technician\"):7, (\"Director\",\"Specialist\"):6, (\"Director\",\"Analyst\"):5, (\"Director\",\"Manager\"):4, (\"Director\",\"Coordinator\"):6, (\"Director\",\"Director\"):0, (\"Director\",\"Other\"):6,\n",
        "    (\"Other\",\"Technician\"):5, (\"Other\",\"Specialist\"):5, (\"Other\",\"Analyst\"):5, (\"Other\",\"Manager\"):5, (\"Other\",\"Coordinator\"):5, (\"Other\",\"Director\"):5, (\"Other\",\"Other\"):0,\n",
        "}\n",
        "\n",
        "# Minor-only severities when majors match: Δ steps {0,1,2,3} → {0,2,3,4}\n",
        "#MINOR_CLASSES = [\"I\",\"II\",\"III\",\"Lead\"]  # keep aligned with your list above (already present)\n",
        "MINOR_IDX = {m:i for i,m in enumerate(MINOR_CLASSES)}\n",
        "\n",
        "# Optional: severity→cost amplification (toggleable)\n",
        "APPLY_SEVERITY_MULTIPLIER = True\n",
        "SEVERITY_COST_MULTIPLIER = {0:0.00, 2:1.15, 3:1.35, 4:1.75, 5:2.50, 6:4.00, 7:6.00}\n",
        "\n",
        "\n",
        "# ==== Column names expected in predictions CSV ====\n",
        "# Adjust here if your Section 4 output uses different names.\n",
        "COL_RECORD_ID   = \"record_id\"\n",
        "COL_TRUE_MAJOR  = \"true_major_role\"\n",
        "COL_PRED_MAJOR  = \"pred_major_role\"\n",
        "COL_TRUE_MINOR  = \"true_minor_role\"\n",
        "COL_PRED_MINOR  = \"pred_minor_role\"\n",
        "\n",
        "# ==== Output directory ====\n",
        "OUTPUT_DIR = \"artifacts_eval_practical\"\n",
        "\n",
        "# --- Google Drive base for inputs (has spaces; that’s fine) ---\n",
        "BASE_INPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data Inputs\"\n",
        "\n",
        "# Point to your actual filenames in that folder:\n",
        "PREDICTIONS_CSV = f\"{BASE_INPUT_DIR}/predictions_batch.csv\"   # <-- change name if different\n",
        "# (If you have other inputs, add them too)\n",
        "# GROUND_TRUTH_CSV = f\"{BASE_INPUT_DIR}/ground_truth.csv\"\n",
        "# MAPPINGS_CSV     = f\"{BASE_INPUT_DIR}/mappings.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78e272b",
      "metadata": {
        "id": "c78e272b"
      },
      "source": [
        "\n",
        "## 3) Helpers\n",
        "\n",
        "Utility functions for loading data, validating columns, computing confusion matrices, error rates, and cost‑of‑error metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46cbc4c",
      "metadata": {
        "id": "d46cbc4c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _require_columns(df: pd.DataFrame, cols):\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}. \"\n",
        "                         f\"Available: {list(df.columns)}\")\n",
        "\n",
        "def load_predictions(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    _require_columns(df, [COL_RECORD_ID, COL_TRUE_MAJOR, COL_PRED_MAJOR, COL_TRUE_MINOR, COL_PRED_MINOR])\n",
        "    return df\n",
        "\n",
        "def confusion(df: pd.DataFrame, true_col: str, pred_col: str, labels: list) -> pd.DataFrame:\n",
        "    cm = pd.crosstab(df[true_col], df[pred_col], rownames=[\"True\"], colnames=[\"Pred\"], dropna=False)\n",
        "    # Ensure all labels exist as rows/cols\n",
        "    cm = cm.reindex(index=labels, columns=labels, fill_value=0)\n",
        "    return cm\n",
        "\n",
        "def accuracy_and_error_rate(df: pd.DataFrame, true_col: str, pred_col: str):\n",
        "    total = len(df)\n",
        "    correct = (df[true_col] == df[pred_col]).sum()\n",
        "    acc = correct / total if total > 0 else float('nan')\n",
        "    err = 1.0 - acc if total > 0 else float('nan')\n",
        "    return acc, err, correct, total\n",
        "\n",
        "def cost_of_error_matrix(df: pd.DataFrame, true_col: str, pred_col: str, cost_map: dict, labels: list) -> pd.DataFrame:\n",
        "    # Build a matrix of summed absolute cost deltas for each (true, pred) pair\n",
        "    # For correct cells (true==pred), cost is 0.\n",
        "    cost_pairs = []\n",
        "    for _, row in df.iterrows():\n",
        "        t, p = row[true_col], row[pred_col]\n",
        "        t_cost = cost_map.get(t, 0.0)\n",
        "        p_cost = cost_map.get(p, 0.0)\n",
        "        delta = abs(t_cost - p_cost) if (t != p) else 0.0\n",
        "        cost_pairs.append((t, p, delta))\n",
        "    cost_df = pd.DataFrame(cost_pairs, columns=[\"True\", \"Pred\", \"CostDelta\"])\n",
        "    pivot = cost_df.pivot_table(index=\"True\", columns=\"Pred\", values=\"CostDelta\", aggfunc=\"sum\", fill_value=0.0)\n",
        "    pivot = pivot.reindex(index=labels, columns=labels, fill_value=0.0)\n",
        "    return pivot\n",
        "\n",
        "def summarize_costs(df: pd.DataFrame, true_col: str, pred_col: str, cost_map: dict):\n",
        "    # Total and average cost of misclassifications\n",
        "    mask_wrong = df[true_col] != df[pred_col]\n",
        "    if mask_wrong.any():\n",
        "        deltas = (df.loc[mask_wrong, true_col].map(cost_map).fillna(0.0) -\n",
        "                  df.loc[mask_wrong, pred_col].map(cost_map).fillna(0.0)).abs()\n",
        "        total_cost = float(deltas.sum())\n",
        "        avg_cost = float(deltas.mean())\n",
        "        n_errors = int(mask_wrong.sum())\n",
        "    else:\n",
        "        total_cost, avg_cost, n_errors = 0.0, 0.0, 0\n",
        "    return {\n",
        "        \"n_errors\": n_errors,\n",
        "        \"total_cost\": total_cost,\n",
        "        \"avg_cost_per_error\": avg_cost\n",
        "    }\n",
        "\n",
        "def export_csv(df: pd.DataFrame, name: str):\n",
        "    out = Path(OUTPUT_DIR) / name\n",
        "    df.to_csv(out, index=True)\n",
        "    return str(out)\n",
        "\n",
        "def export_json(obj, name: str):\n",
        "    out = Path(OUTPUT_DIR) / name\n",
        "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "    return str(out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Helpers for severity, row costs, and pivoting ===\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def minor_delta_severity(true_minor, pred_minor):\n",
        "    ti = MINOR_IDX.get(str(true_minor), None)\n",
        "    pi = MINOR_IDX.get(str(pred_minor), None)\n",
        "    if ti is None or pi is None:\n",
        "        return 0 if ti == pi else 2\n",
        "    d = abs(ti - pi)\n",
        "    return (0, 2, 3, 4)[d] if d <= 3 else 4\n",
        "\n",
        "def severity_score(true_major, true_minor, pred_major, pred_minor):\n",
        "    if true_major == pred_major:\n",
        "        return minor_delta_severity(true_minor, pred_minor)\n",
        "    return SEVERITY_MAJOR_GRID.get((true_major, pred_major), 0)\n",
        "\n",
        "def comp_value(major, minor):\n",
        "    # Composite comp = major base + minor adder (uses your COST_MAPs)\n",
        "    base = COST_MAP_MAJOR.get(major, COST_MAP_MAJOR.get(\"Other\", 0.0))\n",
        "    step = COST_MAP_MINOR.get(str(minor), COST_MAP_MINOR[\"I\"])\n",
        "    return float(base + step)\n",
        "\n",
        "def sev_multiplier(score):\n",
        "    return SEVERITY_COST_MULTIPLIER.get(int(score), 1.0)\n",
        "\n",
        "def add_row_metrics(df):\n",
        "    # Adds: severity_score, true_comp, pred_comp, cost_base, cost_weighted\n",
        "    df = df.copy()\n",
        "    df[\"severity_score\"] = df.apply(lambda r: severity_score(\n",
        "        r[COL_TRUE_MAJOR], r[COL_TRUE_MINOR], r[COL_PRED_MAJOR], r[COL_PRED_MINOR]\n",
        "    ), axis=1)\n",
        "    df[\"true_comp\"] = df.apply(lambda r: comp_value(r[COL_TRUE_MAJOR], r[COL_TRUE_MINOR]), axis=1)\n",
        "    df[\"pred_comp\"] = df.apply(lambda r: comp_value(r[COL_PRED_MAJOR], r[COL_PRED_MINOR]), axis=1)\n",
        "    df[\"cost_base\"] = (df[\"true_comp\"] - df[\"pred_comp\"]).abs()\n",
        "    df[\"cost_weighted\"] = np.where(\n",
        "        APPLY_SEVERITY_MULTIPLIER,\n",
        "        df[\"cost_base\"] * df[\"severity_score\"].map(sev_multiplier),\n",
        "        df[\"cost_base\"]\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def pivot_sum(df, true_col, pred_col, value_col, row_labels, col_labels):\n",
        "    out = pd.DataFrame(0.0, index=row_labels, columns=col_labels)\n",
        "    grp = df.groupby([true_col, pred_col], dropna=False)[value_col].sum()\n",
        "    for (r,c), v in grp.items():\n",
        "        if r in out.index and c in out.columns:\n",
        "            out.loc[r, c] = float(v)\n",
        "    return out\n",
        "\n",
        "def label28(major, minor):\n",
        "    # normalize to known labels\n",
        "    m = major if major in MAJOR_CLASSES else \"Other\"\n",
        "    s = str(minor) if str(minor) in MINOR_CLASSES else \"I\"\n",
        "    return f\"{m} {s}\"\n"
      ],
      "metadata": {
        "id": "oiGjzmFTkhT_"
      },
      "id": "oiGjzmFTkhT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2) Local File Load and Smoke Test"
      ],
      "metadata": {
        "id": "K6FdvJeX7PFB"
      },
      "id": "K6FdvJeX7PFB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your local CSV/XLSX files directly to Colab (one-time per session)\n",
        "from google.colab import files\n",
        "import os, shutil, glob\n",
        "\n",
        "LOCAL_INPUT_DIR = \"local_inputs\"\n",
        "os.makedirs(LOCAL_INPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Select your files (e.g., ground_truth.csv and/or model_outputs.csv):\")\n",
        "uploaded = files.upload()  # Pick files from your computer\n",
        "\n",
        "for name in uploaded.keys():\n",
        "    if os.path.exists(name):\n",
        "        shutil.move(name, os.path.join(LOCAL_INPUT_DIR, name))\n",
        "\n",
        "print(\"\\nFiles now in\", LOCAL_INPUT_DIR)\n",
        "for p in sorted(glob.glob(f\"{LOCAL_INPUT_DIR}/*\")):\n",
        "    print(\" -\", p)\n",
        "\n",
        "# Point the notebook to this folder\n",
        "BASE_INPUT_DIR = LOCAL_INPUT_DIR\n"
      ],
      "metadata": {
        "id": "AIGeCfLR7m_7"
      },
      "id": "AIGeCfLR7m_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===SMOKE TEST====\n",
        "# --- Section 4A: Create a placeholder predictions CSV from ground truth (smoke test) ---\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Point to your labeled truth file on Drive (adjust the name/path)\n",
        "BASE_INPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data Inputs\"\n",
        "TRUTH_CSV      = f\"{BASE_INPUT_DIR}/ground_truth.csv\"     # <-- CHANGE to your actual file\n",
        "\n",
        "# Map your truth columns to the expected names below\n",
        "RECORD_ID_COL  = \"record_id\"          # <-- CHANGE if needed\n",
        "TRUE_MAJOR_COL = \"true_major_role\"    # <-- CHANGE if needed\n",
        "TRUE_MINOR_COL = \"true_minor_role\"    # <-- CHANGE if needed\n",
        "\n",
        "df_truth = pd.read_csv(TRUTH_CSV)\n",
        "\n",
        "# Build a perfect (placeholder) predictions file\n",
        "out = pd.DataFrame({\n",
        "    \"record_id\":        df_truth[RECORD_ID_COL],\n",
        "    \"true_major_role\":  df_truth[TRUE_MAJOR_COL],\n",
        "    \"true_minor_role\":  df_truth[TRUE_MINOR_COL],\n",
        "    \"pred_major_role\":  df_truth[TRUE_MAJOR_COL],   # same as truth (perfect predictions)\n",
        "    \"pred_minor_role\":  df_truth[TRUE_MINOR_COL],\n",
        "})\n",
        "\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "PREDICTIONS_CSV = f\"{OUTPUT_DIR}/predictions_batch.csv\"\n",
        "out.to_csv(PREDICTIONS_CSV, index=False)\n",
        "print(\"✓ Wrote placeholder predictions to:\", PREDICTIONS_CSV)\n"
      ],
      "metadata": {
        "id": "ko2ufZVZYSeY"
      },
      "id": "ko2ufZVZYSeY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df62275a",
      "metadata": {
        "id": "df62275a"
      },
      "source": [
        "\n",
        "## 4) Load Batch Predictions (Section 4 – Unchanged)\n",
        "\n",
        "This section **expects** that you already ran your existing **Section 4** (batch inference).  \n",
        "That step should produce a CSV at `PREDICTIONS_CSV` with the following columns:\n",
        "\n",
        "- `record_id`\n",
        "- `true_major_role` and `pred_major_role` (values in the set: Technician, Specialist, Analyst, Manager, Coordinator, Director, Other)\n",
        "- `true_minor_role` and `pred_minor_role` (values in the set: I, II, III, Lead)\n",
        "\n",
        "> If your column names differ, adjust the constants in Section 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf67dd03",
      "metadata": {
        "id": "bf67dd03"
      },
      "outputs": [],
      "source": [
        "## Section 4 — Load predictions (direct path)\n",
        "import pandas as pd\n",
        "preds = pd.read_csv(PREDICTIONS_CSV)\n",
        "print(f\"Loaded {len(preds):,} predictions from {PREDICTIONS_CSV}\")\n",
        "preds.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==SANITY CHECK==\n",
        "# --- Section 4: quick sanity & normalization (run right after loading `preds`) ---\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) File exists?\n",
        "assert Path(PREDICTIONS_CSV).is_file(), f\"Predictions CSV not found: {PREDICTIONS_CSV}\"\n",
        "\n",
        "# 2) Required columns present?\n",
        "required = [COL_RECORD_ID, COL_TRUE_MAJOR, COL_TRUE_MINOR, COL_PRED_MAJOR, COL_PRED_MINOR]\n",
        "missing = [c for c in required if c not in preds.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Predictions file is missing required column(s): {missing}\\n\"\n",
        "                     f\"Update the COL_* constants in Section 2 or fix the CSV headers.\")\n",
        "\n",
        "# 3) Normalize text columns (avoids whitespace/type surprises)\n",
        "for c in [COL_TRUE_MAJOR, COL_TRUE_MINOR, COL_PRED_MAJOR, COL_PRED_MINOR]:\n",
        "    preds[c] = preds[c].astype(str).str.strip()\n",
        "\n",
        "# 4) Optional: label-set checks (warns if anything is off-spec)\n",
        "unknown_true_major = sorted(set(preds[COL_TRUE_MAJOR]) - set(MAJOR_CLASSES))\n",
        "unknown_pred_major = sorted(set(preds[COL_PRED_MAJOR]) - set(MAJOR_CLASSES))\n",
        "unknown_true_minor = sorted(set(preds[COL_TRUE_MINOR]) - set(MINOR_CLASSES))\n",
        "unknown_pred_minor = sorted(set(preds[COL_PRED_MINOR]) - set(MINOR_CLASSES))\n",
        "\n",
        "if any([unknown_true_major, unknown_pred_major, unknown_true_minor, unknown_pred_minor]):\n",
        "    print(\"⚠️ Label check:\")\n",
        "    print(\"  Unknown TRUE majors:\", unknown_true_major)\n",
        "    print(\"  Unknown PRED majors:\", unknown_pred_major)\n",
        "    print(\"  Unknown TRUE minors:\", unknown_true_minor)\n",
        "    print(\"  Unknown PRED minors:\", unknown_pred_minor)\n",
        "\n",
        "# 5) Peek\n",
        "print(f\"✓ Predictions loaded and validated ({len(preds):,} rows) from {PREDICTIONS_CSV}\")\n",
        "display(preds.head(3))\n"
      ],
      "metadata": {
        "id": "CliLPvF9ZltC"
      },
      "id": "CliLPvF9ZltC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1b789bd4",
      "metadata": {
        "id": "1b789bd4"
      },
      "source": [
        "\n",
        "## 5) Major Role Group Evaluation (Accuracy + Cost of Error)\n",
        "\n",
        "- Compute Top‑1 accuracy and error rate for **Major** groups.\n",
        "- Build a **confusion matrix** to see where misgrouping occurs.\n",
        "- Build a **cost‑of‑error matrix** using the absolute pay delta between true and predicted groups.\n",
        "- Check against **acceptable error** gate: ≤ 2%.\n",
        "\n",
        "Exports:\n",
        "- `confusion_major.csv`\n",
        "- `cost_matrix_major.csv`\n",
        "- `metrics_major.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b9431d",
      "metadata": {
        "id": "c5b9431d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Accuracy/error for Major\n",
        "major_acc, major_err, major_correct, major_total = accuracy_and_error_rate(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR)\n",
        "major_gate_pass = (major_err <= ACCEPTABLE_MAJOR_ERROR_RATE)\n",
        "\n",
        "# Confusion for Major\n",
        "cm_major = confusion(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, MAJOR_CLASSES)\n",
        "cm_major_path = export_csv(cm_major, \"confusion_major.csv\")\n",
        "\n",
        "# Cost-of-error for Major\n",
        "cost_major = cost_of_error_matrix(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, COST_MAP_MAJOR, MAJOR_CLASSES)\n",
        "cost_major_path = export_csv(cost_major, \"cost_matrix_major.csv\")\n",
        "\n",
        "# === Major severity & cost-weighted matrices + 28×28 ===\n",
        "preds_enriched = add_row_metrics(preds)\n",
        "\n",
        "# 7×7 (Major) severity and cost\n",
        "cm_major_sevsum  = pivot_sum(preds_enriched, COL_TRUE_MAJOR, COL_PRED_MAJOR, \"severity_score\", MAJOR_CLASSES, MAJOR_CLASSES)\n",
        "cm_major_sevavg  = cm_major_sevsum / cm_major.replace(0, np.nan)\n",
        "\n",
        "cm_major_costsum = pivot_sum(preds_enriched, COL_TRUE_MAJOR, COL_PRED_MAJOR, \"cost_base\", MAJOR_CLASSES, MAJOR_CLASSES)\n",
        "cm_major_costavg = cm_major_costsum / cm_major.replace(0, np.nan)\n",
        "\n",
        "cm_major_costsum_w = pivot_sum(preds_enriched, COL_TRUE_MAJOR, COL_PRED_MAJOR, \"cost_weighted\", MAJOR_CLASSES, MAJOR_CLASSES)\n",
        "cm_major_costavg_w = cm_major_costsum_w / cm_major.replace(0, np.nan)\n",
        "\n",
        "export_csv(cm_major_sevsum,  \"confusion_major_severity_sum.csv\")\n",
        "export_csv(cm_major_sevavg,  \"confusion_major_severity_avg.csv\")\n",
        "export_csv(cm_major_costsum, \"confusion_major_cost_sum.csv\")\n",
        "export_csv(cm_major_costavg, \"confusion_major_cost_avg.csv\")\n",
        "export_csv(cm_major_costsum_w, \"confusion_major_cost_sum_weighted.csv\")\n",
        "export_csv(cm_major_costavg_w, \"confusion_major_cost_avg_weighted.csv\")\n",
        "\n",
        "# 28×28 labels\n",
        "preds_enriched[\"true_28\"] = preds_enriched.apply(lambda r: label28(r[COL_TRUE_MAJOR], r[COL_TRUE_MINOR]), axis=1)\n",
        "preds_enriched[\"pred_28\"] = preds_enriched.apply(lambda r: label28(r[COL_PRED_MAJOR], r[COL_PRED_MINOR]), axis=1)\n",
        "labels28 = [f\"{M} {m}\" for M in MAJOR_CLASSES for m in MINOR_CLASSES]\n",
        "\n",
        "# 28×28 counts use your existing confusion(); build once to reuse for avg\n",
        "cm_28 = confusion(preds_enriched.rename(columns={\"true_28\":\"__t28\",\"pred_28\":\"__p28\"}), \"__t28\", \"__p28\", labels28)\n",
        "\n",
        "cm_28_sevsum  = pivot_sum(preds_enriched.rename(columns={\"true_28\":\"__t28\",\"pred_28\":\"__p28\"}), \"__t28\",\"__p28\",\"severity_score\", labels28, labels28)\n",
        "cm_28_sevavg  = cm_28_sevsum / cm_28.replace(0, np.nan)\n",
        "\n",
        "cm_28_costsum = pivot_sum(preds_enriched.rename(columns={\"true_28\":\"__t28\",\"pred_28\":\"__p28\"}), \"__t28\",\"__p28\",\"cost_base\", labels28, labels28)\n",
        "cm_28_costavg = cm_28_costsum / cm_28.replace(0, np.nan)\n",
        "\n",
        "cm_28_costsum_w = pivot_sum(preds_enriched.rename(columns={\"true_28\":\"__t28\",\"pred_28\":\"__p28\"}), \"__t28\",\"__p28\",\"cost_weighted\", labels28, labels28)\n",
        "cm_28_costavg_w = cm_28_costsum_w / cm_28.replace(0, np.nan)\n",
        "\n",
        "export_csv(cm_28,             \"confusion_major_minor_counts_28x28.csv\")\n",
        "export_csv(cm_28_sevsum,      \"confusion_major_minor_severity_sum_28x28.csv\")\n",
        "export_csv(cm_28_sevavg,      \"confusion_major_minor_severity_avg_28x28.csv\")\n",
        "export_csv(cm_28_costsum,     \"confusion_major_minor_cost_sum_28x28.csv\")\n",
        "export_csv(cm_28_costavg,     \"confusion_major_minor_cost_avg_28x28.csv\")\n",
        "export_csv(cm_28_costsum_w,   \"confusion_major_minor_cost_sum_weighted_28x28.csv\")\n",
        "export_csv(cm_28_costavg_w,   \"confusion_major_minor_cost_avg_weighted_28x28.csv\")\n",
        "\n",
        "# Summaries\n",
        "cost_summary_major = summarize_costs(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, COST_MAP_MAJOR)\n",
        "\n",
        "metrics_major = {\n",
        "    \"accuracy\": major_acc,\n",
        "    \"error_rate\": major_err,\n",
        "    \"correct\": major_correct,\n",
        "    \"total\": major_total,\n",
        "    \"accept_error_threshold\": ACCEPTABLE_MAJOR_ERROR_RATE,\n",
        "    \"pass_gate\": bool(major_gate_pass),\n",
        "    \"cost_summary\": cost_summary_major,\n",
        "    \"artifacts\": {\n",
        "        \"confusion_major_csv\": cm_major_path,\n",
        "        \"cost_matrix_major_csv\": cost_major_path\n",
        "    }\n",
        "}\n",
        "\n",
        "metrics_major_path = export_json(metrics_major, \"metrics_major.json\")\n",
        "print(\"Major accuracy:\", round(major_acc*100, 2), \"%\")\n",
        "print(\"Major error rate:\", round(major_err*100, 2), \"%\", \"| PASS gate:\", major_gate_pass)\n",
        "print(\"Major cost (total): ${:,.2f} | avg per error: ${:,.2f}\".format(\n",
        "    metrics_major[\"cost_summary\"][\"total_cost\"],\n",
        "    metrics_major[\"cost_summary\"][\"avg_cost_per_error\"],\n",
        "))\n",
        "cm_major\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7599391f",
      "metadata": {
        "id": "7599391f"
      },
      "source": [
        "\n",
        "## 6) Minor Role Group Evaluation (Accuracy + Cost of Error)\n",
        "\n",
        "- Compute Top‑1 accuracy and error rate for **Minor** levels (I, II, III, Lead).\n",
        "- Build a **confusion matrix**.\n",
        "- Build a **cost‑of‑error matrix** using the absolute value difference between true and predicted levels.\n",
        "- Check against **acceptable error** gate: ≤ 5%.\n",
        "\n",
        "Exports:\n",
        "- `confusion_minor.csv`\n",
        "- `cost_matrix_minor.csv`\n",
        "- `metrics_minor.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72999b1c",
      "metadata": {
        "id": "72999b1c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Accuracy/error for Minor\n",
        "minor_acc, minor_err, minor_correct, minor_total = accuracy_and_error_rate(preds, COL_TRUE_MINOR, COL_PRED_MINOR)\n",
        "minor_gate_pass = (minor_err <= ACCEPTABLE_MINOR_ERROR_RATE)\n",
        "\n",
        "# Confusion for Minor\n",
        "cm_minor = confusion(preds, COL_TRUE_MINOR, COL_PRED_MINOR, MINOR_CLASSES)\n",
        "cm_minor_path = export_csv(cm_minor, \"confusion_minor.csv\")\n",
        "\n",
        "# Cost-of-error for Minor\n",
        "cost_minor = cost_of_error_matrix(preds, COL_TRUE_MINOR, COL_PRED_MINOR, COST_MAP_MINOR, MINOR_CLASSES)\n",
        "cost_minor_path = export_csv(cost_minor, \"cost_matrix_minor.csv\")\n",
        "\n",
        "# Summaries\n",
        "cost_summary_minor = summarize_costs(preds, COL_TRUE_MINOR, COL_PRED_MINOR, COST_MAP_MINOR)\n",
        "\n",
        "metrics_minor = {\n",
        "    \"accuracy\": minor_acc,\n",
        "    \"error_rate\": minor_err,\n",
        "    \"correct\": minor_correct,\n",
        "    \"total\": minor_total,\n",
        "    \"accept_error_threshold\": ACCEPTABLE_MINOR_ERROR_RATE,\n",
        "    \"pass_gate\": bool(minor_gate_pass),\n",
        "    \"cost_summary\": cost_summary_minor,\n",
        "    \"artifacts\": {\n",
        "        \"confusion_minor_csv\": cm_minor_path,\n",
        "        \"cost_matrix_minor_csv\": cost_minor_path\n",
        "    }\n",
        "}\n",
        "\n",
        "metrics_minor_path = export_json(metrics_minor, \"metrics_minor.json\")\n",
        "print(\"Minor accuracy:\", round(minor_acc*100, 2), \"%\")\n",
        "print(\"Minor error rate:\", round(minor_err*100, 2), \"%\", \"| PASS gate:\", minor_gate_pass)\n",
        "print(\"Minor cost (total): ${:,.2f} | avg per error: ${:,.2f}\".format(\n",
        "    metrics_minor[\"cost_summary\"][\"total_cost\"],\n",
        "    metrics_minor[\"cost_summary\"][\"avg_cost_per_error\"],\n",
        "))\n",
        "cm_minor\n",
        "\n",
        "# === Minor-only severity summaries (optional but parallel) ===\n",
        "# Here, severity is purely the minor delta (since we’re evaluating minor-level quality)\n",
        "preds_minor = preds_enriched.copy()\n",
        "#cm_minor = confusion(preds_minor, COL_TRUE_MINOR, COL_PRED_MINOR, MINOR_CLASSES)\n",
        "\n",
        "# Minor severity (sum/avg) where majors match or not; still informative at minor level\n",
        "preds_minor[\"minor_severity_only\"] = preds_minor.apply(\n",
        "    lambda r: minor_delta_severity(r[COL_TRUE_MINOR], r[COL_PRED_MINOR]), axis=1\n",
        ")\n",
        "\n",
        "cm_minor_sevsum = pivot_sum(preds_minor, COL_TRUE_MINOR, COL_PRED_MINOR, \"minor_severity_only\", MINOR_CLASSES, MINOR_CLASSES)\n",
        "cm_minor_sevavg = cm_minor_sevsum / cm_minor.replace(0, np.nan)\n",
        "\n",
        "\n",
        "export_csv(cm_minor_sevsum, \"confusion_minor_severity_sum.csv\")\n",
        "export_csv(cm_minor_sevavg, \"confusion_minor_severity_avg.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d6b665",
      "metadata": {
        "id": "18d6b665"
      },
      "source": [
        "\n",
        "## 7) Summary & Review Queue\n",
        "\n",
        "Creates a compact summary of gates and surfaces the most **costly** errors for quick human review.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Section 7 — Summary, KPI, and Review Queues\n",
        "# =========================\n",
        "import pandas as pd\n",
        "\n",
        "# --- KPI: Severity-Weighted Cost Index (SWCI) ---\n",
        "mask_mis = preds_enriched[\"severity_score\"] > 0\n",
        "total_weighted_cost = float(preds_enriched.loc[mask_mis, \"cost_weighted\"].sum())\n",
        "total_payroll       = float(preds_enriched[\"true_comp\"].sum())\n",
        "n_records           = int(len(preds_enriched))\n",
        "mis_rate            = float(mask_mis.mean())\n",
        "\n",
        "# $ per $1M payroll (lower is better) and per 1,000 predictions companion\n",
        "SWCI       = (1_000_000.0 * total_weighted_cost) / max(total_payroll, 1.0)\n",
        "SWCI_per_1k = (1_000.0 * total_weighted_cost) / max(n_records, 1)\n",
        "\n",
        "print(\"=== Severity-Weighted Cost Index ===\")\n",
        "print(f\"SWCI (per $1M payroll):         ${SWCI:,.0f}\")\n",
        "print(f\"Total severity-weighted cost:    ${total_weighted_cost:,.0f}\")\n",
        "print(f\"Total payroll represented:       ${total_payroll:,.0f}\")\n",
        "print(f\"Misclassification rate:          {mis_rate:.2%}\")\n",
        "print(f\"Companion: cost per 1,000 preds: ${SWCI_per_1k:,.0f}\")\n",
        "print(f\"Records evaluated:               {n_records:,}\")\n",
        "\n",
        "# Persist KPI\n",
        "summary_metrics = {\n",
        "    \"SWCI_per_$1M\": SWCI,\n",
        "    \"SWCI_per_1000_predictions\": SWCI_per_1k,\n",
        "    \"total_severity_weighted_cost\": total_weighted_cost,\n",
        "    \"total_payroll\": total_payroll,\n",
        "    \"misclassification_rate\": mis_rate,\n",
        "    \"records\": n_records,\n",
        "    \"severity_multiplier_enabled\": bool(APPLY_SEVERITY_MULTIPLIER),\n",
        "}\n",
        "export_json(summary_metrics, \"summary_metrics.json\")\n",
        "\n",
        "# --- Build overall summary (include KPI) ---\n",
        "summary = {\n",
        "    \"major\": metrics_major,\n",
        "    \"minor\": metrics_minor,\n",
        "    \"gates\": {\n",
        "        \"major_pass\": metrics_major[\"pass_gate\"],\n",
        "        \"minor_pass\": metrics_minor[\"pass_gate\"],\n",
        "    },\n",
        "    \"kpi\": {\n",
        "        \"SWCI_per_$1M\": SWCI,\n",
        "        \"SWCI_per_1000_predictions\": SWCI_per_1k\n",
        "    }\n",
        "}\n",
        "summary_path = export_json(summary, \"summary_overview.json\")\n",
        "\n",
        "# --- Review queues ---\n",
        "def _error_rows_with_cost(df, true_col, pred_col, cost_map, tag):\n",
        "    mask = df[true_col] != df[pred_col]\n",
        "    if not mask.any():\n",
        "        return pd.DataFrame(columns=[\"record_id\", \"true\", \"pred\", \"cost_delta\", \"dimension\"])\n",
        "    t_cost = df.loc[mask, true_col].map(cost_map).fillna(0.0)\n",
        "    p_cost = df.loc[mask, pred_col].map(cost_map).fillna(0.0)\n",
        "    d = (t_cost - p_cost).abs()\n",
        "    out = pd.DataFrame({\n",
        "        \"record_id\": df.loc[mask, COL_RECORD_ID],\n",
        "        \"true\":      df.loc[mask, true_col],\n",
        "        \"pred\":      df.loc[mask, pred_col],\n",
        "        \"cost_delta\": d,\n",
        "        \"dimension\": tag\n",
        "    })\n",
        "    return out\n",
        "\n",
        "# Base review (cost delta) — keep this\n",
        "err_major = _error_rows_with_cost(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, COST_MAP_MAJOR, \"major\")\n",
        "err_minor = _error_rows_with_cost(preds, COL_TRUE_MINOR, COL_PRED_MINOR, COST_MAP_MINOR, \"minor\")\n",
        "review = pd.concat([err_major, err_minor], ignore_index=True)\n",
        "review = review.sort_values(\"cost_delta\", ascending=False).head(200).reset_index(drop=True)\n",
        "review_path = export_csv(review, \"errors_sampled_for_review.csv\")\n",
        "\n",
        "# Severity-weighted review (NEW) — ranks by severity-weighted $ impact\n",
        "review_sw = review.merge(\n",
        "    preds_enriched[[COL_RECORD_ID, \"cost_weighted\"]],\n",
        "    left_on=\"record_id\",\n",
        "    right_on=COL_RECORD_ID,\n",
        "    how=\"left\"\n",
        ").rename(columns={\"cost_weighted\": \"severity_weighted_cost\"})\n",
        "\n",
        "review_sw = review_sw.sort_values([\"severity_weighted_cost\", \"cost_delta\"],\n",
        "                                  ascending=False).head(200).reset_index(drop=True)\n",
        "review_sw_path = export_csv(review_sw, \"errors_sampled_for_review_severity_weighted.csv\")\n",
        "\n",
        "print(\"Summary written to:\", summary_path)\n",
        "print(\"Review (base cost) written to:\", review_path)\n",
        "print(\"Review (severity-weighted) written to:\", review_sw_path)\n",
        "display(review_sw.head(10))\n"
      ],
      "metadata": {
        "id": "BcxwJNcMl-Cf"
      },
      "id": "BcxwJNcMl-Cf",
      "execution_count": null,
      "outputs": []
    }
  ]
}