{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799338ec",
   "metadata": {},
   "source": [
    "\n",
    "# MNPS Practical Evaluation Notebook (Data‑Science Style)\n",
    "\n",
    "**Purpose:** Evaluate model outputs with simple accuracy gates and cost‑of‑error analysis instead of statistical inference.\n",
    "\n",
    "- **Keep Section 4 as-is** (your batch inference that writes predictions to CSV).\n",
    "- **Revised Sections 1–3**: configuration and helpers for the practical evaluation.\n",
    "- **New Sections 5–7**: error thresholds and cost‑of‑error analysis for Major/Minor role groupings.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f9589",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Overview & Acceptance Gates (Engineering Style)\n",
    "\n",
    "We use clear, operational acceptance gates based on what an experienced HR job classification specialist would consider acceptable:\n",
    "\n",
    "- **Major Role Grouping** acceptable error rate: **≤ 2%** (e.g., Analyst vs. Manager misgroup).\n",
    "- **Minor Role Grouping** acceptable error rate: **≤ 5%** (e.g., I vs II vs III vs Lead).\n",
    "\n",
    "We also quantify **severity** via a **cost‑of‑error** analysis:\n",
    "- For **Major** roles, the cost of an error is the **absolute difference** between the annual values mapped to the *true* vs the *predicted* group.\n",
    "- For **Minor** roles, the cost of an error is the **absolute difference** between the values mapped to the *true* vs the *predicted* level.\n",
    "\n",
    "> Rationale: Misclassifying across groups/levels can create pay, equity, and staffing impacts. Using the pay deltas captures the seriousness of the error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949b2b6a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Config & File Paths\n",
    "\n",
    "Set your file paths and constants here. **Do not** change class labels unless your data uses a different spelling; these are the canonical labels used in this evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Required file paths (EDIT as needed) ====\n",
    "PREDICTIONS_CSV = \"data/batch_predictions.csv\"  # produced by Section 4 (unchanged)\n",
    "\n",
    "# ==== Class label vocabularies ====\n",
    "MAJOR_CLASSES = [\"Technician\", \"Specialist\", \"Analyst\", \"Manager\", \"Coordinator\", \"Director\", \"Other\"]\n",
    "MINOR_CLASSES = [\"I\", \"II\", \"III\", \"Lead\"]\n",
    "\n",
    "# ==== Acceptance gates ====\n",
    "ACCEPTABLE_MAJOR_ERROR_RATE = 0.02  # ≤ 2% errors\n",
    "ACCEPTABLE_MINOR_ERROR_RATE = 0.05  # ≤ 5% errors\n",
    "\n",
    "# ==== Cost maps (annual $) ====\n",
    "COST_MAP_MAJOR = {\n",
    "    \"Technician\": 54225.00,\n",
    "    \"Specialist\": 68765.00,\n",
    "    \"Analyst\":   78208.00,\n",
    "    \"Manager\":  103904.00,\n",
    "    \"Coordinator\": 123133.00,\n",
    "    \"Director\": 146246.00,\n",
    "    \"Other\":    0.00,   # If \"Other\" appears, you can adjust as needed.\n",
    "}\n",
    "\n",
    "COST_MAP_MINOR = {\n",
    "    \"I\":    4405.00,\n",
    "    \"II\":   3366.94,\n",
    "    \"III\":  6800.00,\n",
    "    \"Lead\": 6800.00,\n",
    "}\n",
    "\n",
    "# ==== Column names expected in predictions CSV ====\n",
    "# Adjust here if your Section 4 output uses different names.\n",
    "COL_RECORD_ID   = \"record_id\"\n",
    "COL_TRUE_MAJOR  = \"true_major_role\"\n",
    "COL_PRED_MAJOR  = \"pred_major_role\"\n",
    "COL_TRUE_MINOR  = \"true_minor_role\"\n",
    "COL_PRED_MINOR  = \"pred_minor_role\"\n",
    "\n",
    "# ==== Output directory ====\n",
    "OUTPUT_DIR = \"artifacts_eval_practical\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e272b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Helpers\n",
    "\n",
    "Utility functions for loading data, validating columns, computing confusion matrices, error rates, and cost‑of‑error metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cbc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _require_columns(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}. \"\n",
    "                         f\"Available: {list(df.columns)}\")\n",
    "\n",
    "def load_predictions(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    _require_columns(df, [COL_RECORD_ID, COL_TRUE_MAJOR, COL_PRED_MAJOR, COL_TRUE_MINOR, COL_PRED_MINOR])\n",
    "    return df\n",
    "\n",
    "def confusion(df: pd.DataFrame, true_col: str, pred_col: str, labels: list) -> pd.DataFrame:\n",
    "    cm = pd.crosstab(df[true_col], df[pred_col], rownames=[\"True\"], colnames=[\"Pred\"], dropna=False)\n",
    "    # Ensure all labels exist as rows/cols\n",
    "    cm = cm.reindex(index=labels, columns=labels, fill_value=0)\n",
    "    return cm\n",
    "\n",
    "def accuracy_and_error_rate(df: pd.DataFrame, true_col: str, pred_col: str):\n",
    "    total = len(df)\n",
    "    correct = (df[true_col] == df[pred_col]).sum()\n",
    "    acc = correct / total if total > 0 else float('nan')\n",
    "    err = 1.0 - acc if total > 0 else float('nan')\n",
    "    return acc, err, correct, total\n",
    "\n",
    "def cost_of_error_matrix(df: pd.DataFrame, true_col: str, pred_col: str, cost_map: dict, labels: list) -> pd.DataFrame:\n",
    "    # Build a matrix of summed absolute cost deltas for each (true, pred) pair\n",
    "    # For correct cells (true==pred), cost is 0.\n",
    "    cost_pairs = []\n",
    "    for _, row in df.iterrows():\n",
    "        t, p = row[true_col], row[pred_col]\n",
    "        t_cost = cost_map.get(t, 0.0)\n",
    "        p_cost = cost_map.get(p, 0.0)\n",
    "        delta = abs(t_cost - p_cost) if (t != p) else 0.0\n",
    "        cost_pairs.append((t, p, delta))\n",
    "    cost_df = pd.DataFrame(cost_pairs, columns=[\"True\", \"Pred\", \"CostDelta\"])\n",
    "    pivot = cost_df.pivot_table(index=\"True\", columns=\"Pred\", values=\"CostDelta\", aggfunc=\"sum\", fill_value=0.0)\n",
    "    pivot = pivot.reindex(index=labels, columns=labels, fill_value=0.0)\n",
    "    return pivot\n",
    "\n",
    "def summarize_costs(df: pd.DataFrame, true_col: str, pred_col: str, cost_map: dict):\n",
    "    # Total and average cost of misclassifications\n",
    "    mask_wrong = df[true_col] != df[pred_col]\n",
    "    if mask_wrong.any():\n",
    "        deltas = (df.loc[mask_wrong, true_col].map(cost_map).fillna(0.0) - \n",
    "                  df.loc[mask_wrong, pred_col].map(cost_map).fillna(0.0)).abs()\n",
    "        total_cost = float(deltas.sum())\n",
    "        avg_cost = float(deltas.mean())\n",
    "        n_errors = int(mask_wrong.sum())\n",
    "    else:\n",
    "        total_cost, avg_cost, n_errors = 0.0, 0.0, 0\n",
    "    return {\n",
    "        \"n_errors\": n_errors,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"avg_cost_per_error\": avg_cost\n",
    "    }\n",
    "\n",
    "def export_csv(df: pd.DataFrame, name: str):\n",
    "    out = Path(OUTPUT_DIR) / name\n",
    "    df.to_csv(out, index=True)\n",
    "    return str(out)\n",
    "\n",
    "def export_json(obj, name: str):\n",
    "    out = Path(OUTPUT_DIR) / name\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "    return str(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62275a",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Load Batch Predictions (Section 4 – Unchanged)\n",
    "\n",
    "This section **expects** that you already ran your existing **Section 4** (batch inference).  \n",
    "That step should produce a CSV at `PREDICTIONS_CSV` with the following columns:\n",
    "\n",
    "- `record_id`\n",
    "- `true_major_role` and `pred_major_role` (values in the set: Technician, Specialist, Analyst, Manager, Coordinator, Director, Other)\n",
    "- `true_minor_role` and `pred_minor_role` (values in the set: I, II, III, Lead)\n",
    "\n",
    "> If your column names differ, adjust the constants in Section 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = load_predictions(PREDICTIONS_CSV)\n",
    "print(f\"Loaded {len(preds):,} predictions\")\n",
    "preds.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b789bd4",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Major Role Group Evaluation (Accuracy + Cost of Error)\n",
    "\n",
    "- Compute Top‑1 accuracy and error rate for **Major** groups.\n",
    "- Build a **confusion matrix** to see where misgrouping occurs.\n",
    "- Build a **cost‑of‑error matrix** using the absolute pay delta between true and predicted groups.\n",
    "- Check against **acceptable error** gate: ≤ 2%.\n",
    "\n",
    "Exports:\n",
    "- `confusion_major.csv`\n",
    "- `cost_matrix_major.csv`\n",
    "- `metrics_major.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy/error for Major\n",
    "major_acc, major_err, major_correct, major_total = accuracy_and_error_rate(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR)\n",
    "major_gate_pass = (major_err <= ACCEPTABLE_MAJOR_ERROR_RATE)\n",
    "\n",
    "# Confusion for Major\n",
    "cm_major = confusion(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, MAJOR_CLASSES)\n",
    "cm_major_path = export_csv(cm_major, \"confusion_major.csv\")\n",
    "\n",
    "# Cost-of-error for Major\n",
    "cost_major = cost_of_error_matrix(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, COST_MAP_MAJOR, MAJOR_CLASSES)\n",
    "cost_major_path = export_csv(cost_major, \"cost_matrix_major.csv\")\n",
    "\n",
    "# Summaries\n",
    "cost_summary_major = summarize_costs(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, COST_MAP_MAJOR)\n",
    "\n",
    "metrics_major = {\n",
    "    \"accuracy\": major_acc,\n",
    "    \"error_rate\": major_err,\n",
    "    \"correct\": major_correct,\n",
    "    \"total\": major_total,\n",
    "    \"accept_error_threshold\": ACCEPTABLE_MAJOR_ERROR_RATE,\n",
    "    \"pass_gate\": bool(major_gate_pass),\n",
    "    \"cost_summary\": cost_summary_major,\n",
    "    \"artifacts\": {\n",
    "        \"confusion_major_csv\": cm_major_path,\n",
    "        \"cost_matrix_major_csv\": cost_major_path\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_major_path = export_json(metrics_major, \"metrics_major.json\")\n",
    "print(\"Major accuracy:\", round(major_acc*100, 2), \"%\")\n",
    "print(\"Major error rate:\", round(major_err*100, 2), \"%\", \"| PASS gate:\", major_gate_pass)\n",
    "print(\"Major cost (total): ${:,.2f} | avg per error: ${:,.2f}\".format(\n",
    "    metrics_major[\"cost_summary\"][\"total_cost\"],\n",
    "    metrics_major[\"cost_summary\"][\"avg_cost_per_error\"],\n",
    "))\n",
    "cm_major\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599391f",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Minor Role Group Evaluation (Accuracy + Cost of Error)\n",
    "\n",
    "- Compute Top‑1 accuracy and error rate for **Minor** levels (I, II, III, Lead).\n",
    "- Build a **confusion matrix**.\n",
    "- Build a **cost‑of‑error matrix** using the absolute value difference between true and predicted levels.\n",
    "- Check against **acceptable error** gate: ≤ 5%.\n",
    "\n",
    "Exports:\n",
    "- `confusion_minor.csv`\n",
    "- `cost_matrix_minor.csv`\n",
    "- `metrics_minor.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72999b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy/error for Minor\n",
    "minor_acc, minor_err, minor_correct, minor_total = accuracy_and_error_rate(preds, COL_TRUE_MINOR, COL_PRED_MINOR)\n",
    "minor_gate_pass = (minor_err <= ACCEPTABLE_MINOR_ERROR_RATE)\n",
    "\n",
    "# Confusion for Minor\n",
    "cm_minor = confusion(preds, COL_TRUE_MINOR, COL_PRED_MINOR, MINOR_CLASSES)\n",
    "cm_minor_path = export_csv(cm_minor, \"confusion_minor.csv\")\n",
    "\n",
    "# Cost-of-error for Minor\n",
    "cost_minor = cost_of_error_matrix(preds, COL_TRUE_MINOR, COL_PRED_MINOR, COST_MAP_MINOR, MINOR_CLASSES)\n",
    "cost_minor_path = export_csv(cost_minor, \"cost_matrix_minor.csv\")\n",
    "\n",
    "# Summaries\n",
    "cost_summary_minor = summarize_costs(preds, COL_TRUE_MINOR, COL_PRED_MINOR, COST_MAP_MINOR)\n",
    "\n",
    "metrics_minor = {\n",
    "    \"accuracy\": minor_acc,\n",
    "    \"error_rate\": minor_err,\n",
    "    \"correct\": minor_correct,\n",
    "    \"total\": minor_total,\n",
    "    \"accept_error_threshold\": ACCEPTABLE_MINOR_ERROR_RATE,\n",
    "    \"pass_gate\": bool(minor_gate_pass),\n",
    "    \"cost_summary\": cost_summary_minor,\n",
    "    \"artifacts\": {\n",
    "        \"confusion_minor_csv\": cm_minor_path,\n",
    "        \"cost_matrix_minor_csv\": cost_minor_path\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_minor_path = export_json(metrics_minor, \"metrics_minor.json\")\n",
    "print(\"Minor accuracy:\", round(minor_acc*100, 2), \"%\")\n",
    "print(\"Minor error rate:\", round(minor_err*100, 2), \"%\", \"| PASS gate:\", minor_gate_pass)\n",
    "print(\"Minor cost (total): ${:,.2f} | avg per error: ${:,.2f}\".format(\n",
    "    metrics_minor[\"cost_summary\"][\"total_cost\"],\n",
    "    metrics_minor[\"cost_summary\"][\"avg_cost_per_error\"],\n",
    "))\n",
    "cm_minor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6b665",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Summary & Review Queue\n",
    "\n",
    "Creates a compact summary of gates and surfaces the most **costly** errors for quick human review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = {\n",
    "    \"major\": metrics_major,\n",
    "    \"minor\": metrics_minor,\n",
    "    \"gates\": {\n",
    "        \"major_pass\": metrics_major[\"pass_gate\"],\n",
    "        \"minor_pass\": metrics_minor[\"pass_gate\"],\n",
    "    }\n",
    "}\n",
    "summary_path = export_json(summary, \"summary_overview.json\")\n",
    "\n",
    "# Build a review sample of the most costly **major** and **minor** errors (top 200 combined by cost delta).\n",
    "def _error_rows_with_cost(df, true_col, pred_col, cost_map, tag):\n",
    "    mask = df[true_col] != df[pred_col]\n",
    "    if not mask.any():\n",
    "        return pd.DataFrame(columns=[\"record_id\", \"true\", \"pred\", \"cost_delta\", \"dimension\"])\n",
    "    t_cost = df.loc[mask, true_col].map(cost_map).fillna(0.0)\n",
    "    p_cost = df.loc[mask, pred_col].map(cost_map).fillna(0.0)\n",
    "    d = (t_cost - p_cost).abs()\n",
    "    out = pd.DataFrame({\n",
    "        \"record_id\": df.loc[mask, COL_RECORD_ID],\n",
    "        \"true\": df.loc[mask, true_col],\n",
    "        \"pred\": df.loc[mask, pred_col],\n",
    "        \"cost_delta\": d,\n",
    "        \"dimension\": tag\n",
    "    })\n",
    "    return out\n",
    "\n",
    "err_major = _error_rows_with_cost(preds, COL_TRUE_MAJOR, COL_PRED_MAJOR, COST_MAP_MAJOR, \"major\")\n",
    "err_minor = _error_rows_with_cost(preds, COL_TRUE_MINOR, COL_PRED_MINOR, COST_MAP_MINOR, \"minor\")\n",
    "\n",
    "review = pd.concat([err_major, err_minor], ignore_index=True)\n",
    "review = review.sort_values(\"cost_delta\", ascending=False).head(200).reset_index(drop=True)\n",
    "\n",
    "review_path = export_csv(review, \"errors_sampled_for_review.csv\")\n",
    "\n",
    "print(\"Summary written to:\", summary_path)\n",
    "print(\"Review sample written to:\", review_path)\n",
    "review.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
